[{"caption": "Fig. 4. Structure extraction module. SEM is composed of a series of image processing steps, including median filtering, threshold segmentation, connected regions extraction, filling and stroking.", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.6875610351562, "y1": 418.8197021484375, "y2": 431.9019775390625}, "figType": "Figure", "imageText": [], "name": "4", "page": 5, "regionBoundary": {"x1": 116.64, "x2": 478.08, "y1": 280.32, "y2": 407.03999999999996}, "renderDpi": 150, "renderURL": "Figure4-1.png"}, {"caption": "Fig. 5. Convolution blocks used in our proposed IPGN and PCIN. Instance normalization (IN) can accelerate model convergence and promote style consistency between generated images and real images. Transposed convolution layer works well in image reconstruction from reduced representations.", "captionBoundary": {"x1": 37.58700180053711, "x2": 288.6734619140625, "y1": 588.1217041015625, "y2": 618.3399658203125}, "figType": "Figure", "imageText": [], "name": "5", "page": 5, "regionBoundary": {"x1": 44.64, "x2": 281.28, "y1": 447.35999999999996, "y2": 576.0}, "renderDpi": 150, "renderURL": "Figure5-1.png"}, {"caption": "Fig. 3. Visualization of holes generation algorithm. (a\u223cc) shows some images from  2264,  3 282 and  4 215 respectively. Green color represents minimum boundary rectangle of left lung and right lung. Red color represents generated holes and their center points. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.6884765625, "y1": 242.73968505859375, "y2": 264.38897705078125}, "figType": "Figure", "imageText": [], "name": "3", "page": 5, "regionBoundary": {"x1": 152.64, "x2": 442.08, "y1": 55.68, "y2": 230.88}, "renderDpi": 150, "renderURL": "Figure3-1.png"}, {"caption": "Table 6 Comparison of the deep learning models used in this segmentation process in terms of number of training parameters and FLOPs.", "captionBoundary": {"x1": 37.58700180053711, "x2": 288.674560546875, "y1": 316.2767028808594, "y2": 337.927001953125}, "figType": "Table", "imageText": ["IPGN", "GAN", "1.61M", "16.17G", "PCIN", "VGG16", "25.46M", "236.94G", "JCS", "VGG16&Res2net101", "117.72M", "484.14G", "COVID-rate", "\u2013", "14.38M", "297.92G", "AnamNet", "\u2013", "4.47M", "196.12G", "MiniSeg", "\u2013", "0.8M", "1.03G", "InfNet", "\u2013", "31.07M", "57.56G", "UNet", "\u2013", "31.04M", "421.49G", "Attention-UNet", "UNet", "34.87M", "513.79G", "SegNet", "VGG", "29.44M", "422.07M", "Methods", "Baseline", "#Param", "FLOPs"], "name": "6", "page": 10, "regionBoundary": {"x1": 36.96, "x2": 289.44, "y1": 340.32, "y2": 448.8}, "renderDpi": 150, "renderURL": "Table6-1.png"}, {"caption": "Fig. 13. Visual results of PCIN and four existing methods. GT represents ground truth. PCIN generates most reasonable and realistic content.", "captionBoundary": {"x1": 90.3280029296875, "x2": 504.9456481933594, "y1": 280.03369140625, "y2": 284.5479736328125}, "figType": "Figure", "imageText": [], "name": "13", "page": 10, "regionBoundary": {"x1": 86.88, "x2": 508.32, "y1": 55.68, "y2": 267.84}, "renderDpi": 150, "renderURL": "Figure13-1.png"}, {"caption": "Fig. 1. (a) Input CT images with missing rectangle holes (drawn in red). (b) Structure of CT images recovered from structure inpainting branch of PCIN. (c) Complete results1 from comprehensive inpainting branch of PCIN. (d) Ground truth. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.6876220703125, "y1": 193.9237060546875, "y2": 215.573974609375}, "figType": "Figure", "imageText": [], "name": "1", "page": 1, "regionBoundary": {"x1": 146.88, "x2": 448.32, "y1": 55.68, "y2": 181.92}, "renderDpi": 150, "renderURL": "Figure1-1.png"}, {"caption": "Fig. 6. The structure of initial patch generation network. IPGN generates fake images of 100 \u00d7 100 from 2D random noises of 25 \u00d7 25. A discriminator is designed and utilized an adversarial loss to train IPGN.", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.687744140625, "y1": 234.08868408203125, "y2": 247.17095947265625}, "figType": "Figure", "imageText": [], "name": "6", "page": 6, "regionBoundary": {"x1": 86.88, "x2": 508.32, "y1": 55.68, "y2": 221.76}, "renderDpi": 150, "renderURL": "Figure6-1.png"}, {"caption": "Fig. 7. Comparison between (a) real images and (b) IPGN generated fake images.", "captionBoundary": {"x1": 42.74399948120117, "x2": 283.51611328125, "y1": 379.6067199707031, "y2": 384.1210021972656}, "figType": "Figure", "imageText": [], "name": "7", "page": 6, "regionBoundary": {"x1": 37.92, "x2": 288.0, "y1": 261.59999999999997, "y2": 368.15999999999997}, "renderDpi": 150, "renderURL": "Figure7-1.png"}, {"caption": "Fig. 11. Performance of Faster RCNN. (a) and (b) are Precision and Recall under different IoU thresholds, respectively. (c) shows the Precision\u2013Recall (PR) curves.", "captionBoundary": {"x1": 57.82099914550781, "x2": 537.4553833007812, "y1": 162.0997314453125, "y2": 166.614013671875}, "figType": "Figure", "imageText": [], "name": "11", "page": 9, "regionBoundary": {"x1": 123.83999999999999, "x2": 471.35999999999996, "y1": 55.68, "y2": 149.76}, "renderDpi": 150, "renderURL": "Figure11-1.png"}, {"caption": "Fig. 12. Visual results of lesion detection using Faster RCNN. The bounding boxes of lesion and their prediction possibilities are shown in red color. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.6866455078125, "y1": 316.7567138671875, "y2": 329.8379821777344}, "figType": "Figure", "imageText": [], "name": "12", "page": 9, "regionBoundary": {"x1": 118.56, "x2": 477.12, "y1": 198.23999999999998, "y2": 304.8}, "renderDpi": 150, "renderURL": "Figure12-1.png"}, {"caption": "Table 5 Comparison of image inpainting performance for different ratios of missing area.", "captionBoundary": {"x1": 115.60299682617188, "x2": 351.9591979980469, "y1": 362.27471923828125, "y2": 375.35699462890625}, "figType": "Table", "imageText": ["EC", "0.6712", "0.6043", "0.4931", "0.3727", "0.2643", "0.1282", "CA", "0.5686", "0.4305", "0.3423", "0.2633", "0.2226", "0.0418", "PC", "0.6914", "0.6185", "0.4902", "0.3731", "0.2959", "0.1126", "GC", "0.3551", "0.4041", "0.3039", "0.2966", "0.3666", "0.2769", "PCIN", "0.6435", "0.6378", "0.6528", "0.6419", "0.6845", "0.6634", "\ud835\udc46\ud835\udc46\ud835\udc3c\ud835\udc40+", "EC", "13.93", "10.53", "9.08", "8.17", "7.18", "6.6", "CA", "17.05", "10.84", "8.02", "6.41", "4.88", "4.4", "PC", "18.31", "13.37", "10.67", "7.93", "5.29", "4.31", "GC", "15.14", "12.05", "9.42", "8.38", "7.21", "6.29", "PCIN", "27.46", "24.1", "26.94", "26.11", "25.54", "23.39", "\ud835\udc43\ud835\udc46\ud835\udc41\ud835\udc45+", "EC", "32.51", "48.05", "63.74", "76.55", "85.24", "95.17", "CA", "19.98", "39.21", "61.35", "81.46", "103.81", "122.43", "PC", "17.89", "30.03", "44.11", "62.36", "95.19", "109.43", "GC", "28.43", "39.67", "55.01", "68.47", "79.19", "96.81", "PCIN", "7.27", "7.22", "7.02", "7.65", "6.63", "6.19", "\ud835\udc40\ud835\udc34\ud835\udc38\u2212", "Ratio", "0\u223c2%", "2\u223c4%", "4\u223c6%", "6\u223c8%", "8%\u223c10%", ">10%"], "name": "5", "page": 9, "regionBoundary": {"x1": 114.72, "x2": 480.47999999999996, "y1": 377.28, "y2": 529.92}, "renderDpi": 150, "renderURL": "Table5-1.png"}, {"caption": "Fig. 8. The structure of progressive CT inpainting network. PCIN decouples the recovery of skeleton from the overall recovery through structure inpainting branch. Deep structure features with different scales extracted from structure inpainting branch are fused to comprehensive inpainting branch for a reasonable result generation.", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.687744140625, "y1": 255.9556884765625, "y2": 269.0379638671875}, "figType": "Figure", "imageText": [], "name": "8", "page": 7, "regionBoundary": {"x1": 86.88, "x2": 508.32, "y1": 57.599999999999994, "y2": 243.84}, "renderDpi": 150, "renderURL": "Figure8-1.png"}, {"caption": "Fig. 10. Statistics of the bounding boxes of lesion in the training set. (a) Count distribution. (b) Size distribution. (c) Distribution of lesion location. (d) Aspect ratio distribution. Zoom in for details.", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.6890258789062, "y1": 724.5477294921875, "y2": 737.6300048828125}, "figType": "Figure", "imageText": [], "name": "10", "page": 7, "regionBoundary": {"x1": 138.72, "x2": 456.47999999999996, "y1": 454.56, "y2": 712.8}, "renderDpi": 150, "renderURL": "Figure10-1.png"}, {"caption": "Fig. 9. Details of segmentation, which consists of blood vessels removal, image difference and a series of post-processing steps.", "captionBoundary": {"x1": 110.47200012207031, "x2": 484.8029479980469, "y1": 432.3507080078125, "y2": 436.864990234375}, "figType": "Figure", "imageText": [], "name": "9", "page": 7, "regionBoundary": {"x1": 86.88, "x2": 508.32, "y1": 286.56, "y2": 420.96}, "renderDpi": 150, "renderURL": "Figure9-1.png"}, {"caption": "Table 1 Initial setting of anchors.", "captionBoundary": {"x1": 315.1239929199219, "x2": 388.19476318359375, "y1": 459.3817138671875, "y2": 472.4629821777344}, "figType": "Table", "imageText": ["Anchor", "size", "(26,", "39),", "(32", "32),", "(45", "22),", "(105", "157),", "(128", "128),", "(width,", "height)", "(181", "90),", "(209", "314),", "(256", "256),", "(362", "181)", "Base", "size", "8", "Scale", "(4,", "16,", "32)", "Aspect", "ratio", "(0.5,", "1,", "1.5)"], "name": "1", "page": 3, "regionBoundary": {"x1": 314.88, "x2": 550.0799999999999, "y1": 474.24, "y2": 526.0799999999999}, "renderDpi": 150, "renderURL": "Table1-1.png"}, {"caption": "Fig. 2. Workflow of our proposed weakly supervised inpainting-based learning method for COVID-19 lesion segmentation. In the (a) training period, positive samples \ud835\udc5d are used for the training of Faster RCNN. Dataset for PCIN is made by generating rectangle holes on negative images \ud835\udc5b based on real information of the bounding boxes of lesion from \ud835\udc5d. Image patches \ud835\udc54 cropped from \ud835\udc5b are utilized to train IPGN. (b) Inference period consists of three stages: detection, inpainting and segmentation.", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.6887817382812, "y1": 420.0027160644531, "y2": 443.2469787597656}, "figType": "Figure", "imageText": [], "name": "2", "page": 3, "regionBoundary": {"x1": 102.72, "x2": 492.0, "y1": 55.68, "y2": 408.0}, "renderDpi": 150, "renderURL": "Figure2-1.png"}, {"caption": "Table 7 Comparison of lesion segmentation performance.", "captionBoundary": {"x1": 154.60899353027344, "x2": 296.246337890625, "y1": 55.791690826416016, "y2": 68.87396240234375}, "figType": "Table", "imageText": ["Bounding", "Box", "GrabCut", "43.87", "43.42", "43.61", "99.83", "Ours", "78.34", "70.82", "82.95", "99.75", "Full", "AnamNet", "81.62", "75.08", "79.95", "99.92", "MiniSeg", "81.05", "74.72", "80.53", "99.90", "InfNet", "85.88", "79.93", "87.01", "99.89", "JCS", "82.33", "76.38", "81.95", "99.90", "COVID-rate", "81.96", "75.68", "84.67", "99.85", "SegNet", "76.82", "69.95", "74.88", "99.91", "UNet", "77.23", "70.41", "77.05", "99.91", "Full", "Attention-UNet", "80.22", "73.81", "79.18", "99.91", "Label", "Methods", "\ud835\udc37\ud835\udc46\ud835\udc36(%)", "\ud835\udc3c\ud835\udc5c\ud835\udc48", "(%)", "\ud835\udc46\ud835\udc38\ud835\udc41(%)", "\ud835\udc46\ud835\udc43\ud835\udc38(%)"], "name": "7", "page": 11, "regionBoundary": {"x1": 153.6, "x2": 441.12, "y1": 71.52, "y2": 180.0}, "renderDpi": 150, "renderURL": "Table7-1.png"}, {"caption": "Fig. 14. Visual comparison of lesion segmentation results. GT represents ground truth. The green, blue, and red regions refer to true positive, false negative and false positive pixels, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)", "captionBoundary": {"x1": 37.58700180053711, "x2": 557.6905517578125, "y1": 407.04071044921875, "y2": 420.1219787597656}, "figType": "Figure", "imageText": [], "name": "14", "page": 11, "regionBoundary": {"x1": 86.88, "x2": 508.32, "y1": 194.4, "y2": 395.03999999999996}, "renderDpi": 150, "renderURL": "Figure14-1.png"}, {"caption": "Table 8 Comparison of segmentation performance for inpainting networks.", "captionBoundary": {"x1": 37.58700180053711, "x2": 230.7834014892578, "y1": 451.8297119140625, "y2": 464.9109802246094}, "figType": "Table", "imageText": ["PCIN", "74.28", "68.22", "73.01", "99.50", "PCIN(Post-Processing)", "78.34", "70.82", "78.07", "99.75", "EC", "72.53", "65.91", "77.79", "99.46", "CA", "69.89", "61.68", "72.08", "99.49", "PC", "68.10", "59.65", "69.25", "99.46", "GC", "72.29", "64.23", "77.08", "99.42", "Fast", "Marching", "67.20", "58.82", "68.29", "99.51", "Navier\u2013Stokes", "67.39", "58.98", "68.05", "99.55", "Methods", "\ud835\udc37\ud835\udc46\ud835\udc36(%)", "\ud835\udc3c\ud835\udc5c\ud835\udc48", "(%)", "\ud835\udc46\ud835\udc38\ud835\udc41(%)", "\ud835\udc46\ud835\udc43\ud835\udc38(%)"], "name": "8", "page": 11, "regionBoundary": {"x1": 36.96, "x2": 289.44, "y1": 467.52, "y2": 558.72}, "renderDpi": 150, "renderURL": "Table8-1.png"}, {"caption": "Table 4 Comparison of inpainting performance with different methods.", "captionBoundary": {"x1": 328.12799072265625, "x2": 510.65283203125, "y1": 55.791690826416016, "y2": 68.87396240234375}, "figType": "Table", "imageText": ["PCIN(\u2217):", "\u2217", "represents", "initial", "value", "of", "missing", "holes.", "\ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b:", "The", "mean", "pixel", "value", "of", "the", "training", "set.", "PCIN(0)", "21.23", "17.87", "0.3042", "PCIN(127)", "23.63", "17.58", "0.3021", "PCIN(255)", "24.83", "17.04", "0.2889", "PCIN(\ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b)", "23.47", "17.61", "0.3032", "PCIN(\ud835\udc53\ud835\udc56\ud835\udc5d\ud835\udc54\ud835\udc5b)", "14.93", "21.47", "0.4182", "EC", "81.08", "7.33", "0.1224", "CA", "49.89", "12.01", "0.1784", "PC", "63.97", "10.27", "0.1101", "GC", "93.12", "7.49", "0.1263", "Methods", "\ud835\udc40\ud835\udc34\ud835\udc38\u2212", "\ud835\udc43\ud835\udc46\ud835\udc41\ud835\udc45+", "\ud835\udc46\ud835\udc46\ud835\udc3c\ud835\udc40+"], "name": "4", "page": 8, "regionBoundary": {"x1": 327.84, "x2": 537.12, "y1": 71.52, "y2": 185.76}, "renderDpi": 150, "renderURL": "Table4-1.png"}, {"caption": "Table 2 Details of the public dataset.", "captionBoundary": {"x1": 59.111000061035156, "x2": 142.96945190429688, "y1": 55.791690826416016, "y2": 68.87396240234375}, "figType": "Table", "imageText": ["Train", "2508", "1720", "4228", "Test", "496", "158", "654", "Total", "3004", "1878", "4882", "Positive", "Negative", "Total"], "name": "2", "page": 8, "regionBoundary": {"x1": 58.559999999999995, "x2": 268.32, "y1": 71.52, "y2": 113.75999999999999}, "renderDpi": 150, "renderURL": "Table2-1.png"}, {"caption": "Table 3 Detail parameters of training.", "captionBoundary": {"x1": 59.111000061035156, "x2": 144.97471618652344, "y1": 128.9766845703125, "y2": 142.0589599609375}, "figType": "Table", "imageText": ["Faster", "RCNN", "6", "100", "1e\u22125", "IPGN", "20", "100", "2e\u22124", "PCIN", "2", "100", "2e-3", "Network", "Batch", "size", "Epochs", "Learning", "rate"], "name": "3", "page": 8, "regionBoundary": {"x1": 58.559999999999995, "x2": 268.32, "y1": 144.48, "y2": 186.72}, "renderDpi": 150, "renderURL": "Table3-1.png"}]