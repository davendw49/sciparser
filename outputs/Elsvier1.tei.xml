<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A weakly supervised inpainting-based learning method for lung CT image segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Elsevier BV</publisher>
				<availability status="unknown"><p>Copyright Elsevier BV</p>
				</availability>
				<date type="published" when="2023-08-06">6 August 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,37.59,205.05,57.24,9.84"><forename type="first">Fangfang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, 201306, China</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>201306</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,106.24,205.05,64.21,9.84"><forename type="first">Zhihao</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0002-6936-3999</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, 201306, China</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>201306</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.88,205.05,63.84,9.84"><forename type="first">Tianxiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, 201306, China</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>201306</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.15,205.05,41.77,9.84"><forename type="first">Chi</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, 201306, China</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>201306</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.34,205.05,49.17,9.84"><forename type="first">Hualin</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, 201306, China</note>
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>201306</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.93,205.05,68.32,9.84"><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>b</label> Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China</note>
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,37.59,218.00,64.18,9.84"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
							<idno type="ORCID">0000-0003-1737-3420</idno>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>c</label> School of economics, Fudan University, Shanghai, 200433, China</note>
								<orgName type="department">School of economics</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,118.32,218.00,54.08,9.84"><forename type="first">Xiaoxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>d</label> The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, Zhejiang Province, 310003, China</note>
								<orgName type="institution" key="instit1">The First Affiliated Hospital</orgName>
								<orgName type="institution" key="instit2">Zhejiang University School of Medicine</orgName>
								<address>
									<addrLine>Zhejiang Province</addrLine>
									<postCode>310003</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A weakly supervised inpainting-based learning method for lung CT image segmentation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Pattern Recognition</title>
						<title level="j" type="abbrev">Pattern Recognition</title>
						<idno type="ISSN">0031-3203</idno>
						<imprint>
							<publisher>Elsevier BV</publisher>
							<biblScope unit="volume">144</biblScope>
							<biblScope unit="page">109861</biblScope>
							<date type="published" when="2023-08-06">6 August 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">C17DB47570CD402DE009909BA0818296</idno>
					<idno type="DOI">10.1016/j.patcog.2023.109861</idno>
					<note type="submission">Received 12 July 2022; Received in revised form 12 July 2023; Accepted 31 July 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-09-03T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>COVID-19 Weakly supervised Lesion segmentation Image inpainting</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, various fully supervised learning methods are successfully applied for lung CT image segmentation. However, pixel-wise annotations are extremely expert-demanding and labor-intensive, but the performance of unsupervised learning methods are failed to meet the demands of practical applications. To achieve a reasonable trade-off between the performance and label dependency, a novel weakly supervised inpaintingbased learning method is introduced, in which only bounding box labels are required for accurate segmentation. Specifically, lesion regions are first detected by an object detection network, then we crop them out of the input image and recover the missing holes to normal regions using a progressive CT inpainting network (PCIN). Finally, a post-processing method is designed to get the accurate segmentation mask from the difference image of input and recovered images. In addition, real information (i.e., number, location and size) of the bounding boxes of lesions from the dataset guides us to make the training dataset for PCIN. We apply a multi-scale supervised strategy to train PCIN for a progressive and stable inpainting. Moreover, to remove the visual artifacts resulted from the invalid features of missing holes, an initial patch generation network (IPGN) is proposed for holes initialization with generated pseudo healthy image patches. Experiments on the public COVID-19 dataset demonstrate that PCIN is outstanding in lung CT images inpainting, and the performance of our proposed weakly supervised method is comparable to fully supervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>According to the latest statistics from the Center for Systems Science and Engineering (CSSE) of Johns Hopkins University (JHU) (updated <ref type="bibr" coords="1,37.59,513.20,61.39,7.43">March 10, 2023)</ref>, confirmed cases of COVID-19 in the world have reached 676.6 million, including 6.88 million deaths. Recently, with the wide spread of Omicron mutant in the world, the epidemic has rebounded in many countries and newly confirmed cases per day have reached a new high.</p><p>Although real-time reverse transcription-polymerase chain reaction (RT-PCR) is considered the diagnostic benchmark for COVID-19, it suffers from many limitation such as short in supply, high false-negative rates and higher cost equipment <ref type="bibr" coords="1,164.57,598.06,10.09,7.43" target="#b0">[1]</ref>. Chest computed tomography (CT) images of COVID-19 patients have unique characteristics, which show multiple small patchy shadows and interstitial changes, mainly in the peripheral lungs at the early stage and then multiple groundglass opaque (GGO) infiltrates in both lungs <ref type="bibr" coords="1,199.58,640.49,10.09,7.43" target="#b1">[2]</ref>. Clinical practice has of these methods is considered directly proportional to the scale and accuracy of the required dataset. It should be noted that annotation of pixel-level labels is a labor-intensive process and suffers from a high cost and uneven quality. Although the unsupervised learning methods are label-free, their performance fail to meet the demands of practical applications. Semi-supervised and weakly supervised methods are able to achieve a trade-off between the performance and label dependency, however, few related researches exist.</p><p>Weakly supervised learning methods for semantic segmentation, can be divided into image-level label-based, bounding box label-based, scribble label-based and points label-based according to the way of annotation. Image-level labels only provide the category of an image, so image-level label-based methods first train a classification model, then employ Class Activation Maps (CAM) <ref type="bibr" coords="2,170.39,368.75,11.23,7.43" target="#b5">[6]</ref> technology to obtain the most responsive regions from the model as seed regions, finally perform a series of expansion methods to obtain the segmentation mask. Because CAM are unable to cover the entire target, it requires a high performance of the post-processing steps to ensure final segmentation. The bounding boxes are the representations of objects position, so bounding box label-based methods treat it as a strong topological prior, then combine traditional image algorithm GrabCut <ref type="bibr" coords="2,200.39,442.29,11.23,7.43" target="#b6">[7]</ref> or its improved trainable version DeepCut <ref type="bibr" coords="2,118.11,452.80,10.09,7.43" target="#b7">[8]</ref>. Scribble labels refer to marking the target with scribble lines to get the rough location information. Scribble labelbased methods <ref type="bibr" coords="2,95.75,473.81,11.22,7.43" target="#b8">[9,</ref><ref type="bibr" coords="2,106.97,473.81,11.22,7.43" target="#b9">10]</ref> are extension of DeepCut, which incorporate deep features in constructing the unary part of the loss function. Points labels indicate the object location using single or few pixels. Points label-based method <ref type="bibr" coords="2,107.98,505.33,15.72,7.43" target="#b10">[11]</ref> is prone to local minima, e.g., focusing on only a small part of the target. Therefore, objectness prior is included in the training loss function to accurately infer the object extent. These four methods have achieved passable results on public datasets such as MS COCO and PASCAL VOC. All other weakly label-based methods except bounding box label-based methods have been applied for COVID-19 lesion segmentation.</p><p>To reduce the dependence on pixel-level labels, in this paper, we explore a novel weakly supervised inpainting-based learning method for COVID-19 lesion segmentation, in which only bounding box labels are required for training. Different from the general practice, our proposed method consists of three stages: detection, inpainting and segmentation. In the first stage, the bounding boxes of lesions are detected by the state-of-the-art object detection network Faster RCNN <ref type="bibr" coords="2,62.49,652.51,14.36,7.43" target="#b11">[12]</ref>. In the second stage, we crop the detected lesion regions out of the image and recover the missing holes to normal regions using image inpainting technology. In the third stage, the segmentation mask is obtained from the difference image between the input and recovered images through a series of post-processing operations 1 We did not overlay the results with the region of no missing to comprehensively demonstrate our inpainting effect. including median filtering, binarization, dilation and region growing. For a better image inpainting, our proposed progressive CT inpainting network (PCIN) decouples the recovery of high-frequency and lowfrequency information. As shown in Fig. <ref type="figure" coords="2,461.32,263.59,3.36,7.43" target="#fig_0">1</ref>, PCIN first recovers the missing skeleton (Fig. <ref type="figure" coords="2,384.99,274.10,10.74,7.43" target="#fig_0">1(b</ref>)) using structure inpainting branch, and then utilizes the comprehensive inpainting branch for a reasonable complete result (Fig. <ref type="figure" coords="2,347.84,295.11,10.31,7.43" target="#fig_0">1(c</ref>)) generation. To solve the issue that invalid features of the holes lead to color discrepancy and blurriness, we propose an initial patch generation network (IPGN) to generate pseudo healthy image patches for holes initialization. In addition, we supervise the recovery effect at multiple scales during training period for a progressive and stable inpainting.</p><p>Our contributions are summarized as follows:</p><p>1. We propose a weakly supervised inpainting-based learning method for COVID-19 lesion segmentation from lung CT images, in which only bounding box labels are required and three stages: detection, inpainting and segmentation are included. 2. A progressive CT inpainting network (PCIN) is proposed to recover lung CT images with rectangle missing holes, which first uses structure inpainting branch to recover the skeleton, and then fuses deep structure features to comprehensive inpainting branch for a reasonable complete result generation. 3. A multi-scale supervised strategy is applied to train PCIN for a progressive and stable inpainting. In addition, to remove the visual artifacts resulted from the invalid features of missing holes, an initial patch generation network (IPGN) is proposed to generate pseudo healthy image patches for holes initialization. 4. Experiments on public COVID-19 dataset demonstrate that PCIN is outstanding in CT images inpainting, and the performance of our proposed weakly supervised method is comparable to fully supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section, we briefly review deep learning-based methods for COVID-19 lesion segmentation and image inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep learning-based methods for COVID-19 lesion segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Fully supervised learning methods</head><p>UNet <ref type="bibr" coords="2,340.48,661.05,15.72,7.43" target="#b12">[13]</ref> is generally acknowledged as a fully supervised standard backbone network for medical image segmentation. UNet and its improvement networks (UNet++ <ref type="bibr" coords="2,441.48,682.06,14.36,7.43" target="#b13">[14]</ref>, VNet <ref type="bibr" coords="2,484.72,682.06,14.36,7.43" target="#b14">[15]</ref>, etc.) or UNetlike networks are also widely used for COVID-19 lesion segmentation because of their excellent performance. Cao et al. <ref type="bibr" coords="2,495.32,703.07,11.23,7.43" target="#b2">[3]</ref> used original UNet to segment the COVID-19 lesion from CT images. Jin et al. <ref type="bibr" coords="2,546.46,713.58,11.23,7.43" target="#b4">[5]</ref> used UNet++ for lesion segmentation. To improve the segmentation performance, Yan et al. <ref type="bibr" coords="2,398.56,734.59,15.72,7.43" target="#b15">[16]</ref> combined the Atrous Spatial Pyramid Pooling (ASPP) module to UNet to increase the receptive fields of network as well as improve the ability of multi-scale contexts acquirement. Fully supervised methods work well but rely on large-scale accurate pixel-level label datasets, whereas only small-scale datasets are available for researchers. To achieve comparable performance, multi-input and multi-scale inputs are used to adequately extract image features and perform feature fusion. In addition, researches on improving the loss function are also presented. Wang et al. <ref type="bibr" coords="3,206.98,129.13,15.72,7.43" target="#b16">[17]</ref> proposed a noise robust Dice loss function, which solves the problem of poor training performance with low quality and high noise labels. To address the overfitting caused by millions of model parameters, Anam-Net <ref type="bibr" coords="3,272.95,160.45,15.72,7.43" target="#b17">[18]</ref> and MiniSeg <ref type="bibr" coords="3,85.93,170.89,15.72,7.43" target="#b18">[19]</ref> are proposed, which also promote the deployment of models on embedded devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Weakly supervised learning methods</head><p>Statistics <ref type="bibr" coords="3,84.15,212.63,15.72,7.43" target="#b10">[11]</ref> show that making the accurate pixel-level segmentation labels is so labor-intensive that it takes 239.0 s per image on average for the PASCAL VOC dataset, whereas collecting image-level and point-level labels takes only 20.0 and 22.1 s per image. Hence, different forms of weaker labels are applied for lesion segmentation, including image-level, bounding boxes-level, scribble-level and pointlevel annotations. Wang et al. <ref type="bibr" coords="3,145.48,275.27,15.72,7.43" target="#b19">[20]</ref> first inferred some candidate lesion regions as seeds from classification model trained from image-level labels by applying CAM method, then performed a series of expansion methods to obtain the segmentation mask. However, CAM only focuses on the most discernible regions but not covers the entire target, which requires a high performance of the post-processing algorithm to ensure the segmentation result. Liu <ref type="bibr" coords="3,139.05,337.91,15.72,7.43" target="#b20">[21]</ref> trained a COVID-19 lesion segmentation model using scribble-level labels. To deal with the difficulty caused by the shortage of supervision, he incorporated uncertainty-aware selfensembling and transformation-consistent techniques. Point-level labels contain little supervised information and thus are most difficult to be exploit. To resolve the low precision of the model, Laradji et al. <ref type="bibr" coords="3,272.96,390.12,15.72,7.43" target="#b21">[22]</ref> proposed a consistency-based loss function that encourages the output predictions to be consistent with spatial transformations of the input CT images. To the best of our knowledge, we firstly propose a weakly supervised learning method for COVID-19 lesion segmentation using bounding boxes-level labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Semi-supervised learning methods</head><p>The missing Labels of some images in the dataset result that networks fail to calculate their losses in training period, so semi-supervised methods focus on how to get the unlabeled image into training. Currently, two types of solutions have been proposed to solve this problem, including consistent regularization and self-training using pseudo labels. The first type of solutions follows the assumption that the inputs under different perturbations would be predicted the same result. Thus, we are able to apply perturbations like noise, rotation and scaling to input CT images, or apply perturbations like dropout to networks, then encourage networks to be transformation consistent for unlabeled images. Ding et al. <ref type="bibr" coords="3,114.67,578.01,15.72,7.43" target="#b22">[23]</ref> applied this strategy to COVID-19 lesion segmentation. The second type of solutions first obtains the pseudo labels for unlabeled CT images from the model trained from labeled CT images. Pseudo labels are then used to supervise the unlabeled CT images in training period. Training dataset are gradually enlarged by unlabeled images with reliable pseudo labels. Meanwhile, the model results and pseudo labels gradually become more accurate. Fan et al. <ref type="bibr" coords="3,272.96,640.66,15.72,7.43" target="#b23">[24]</ref> applied this strategy for COVID-19 lesion segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">Unsupervised learning methods</head><p>Unsupervised methods are currently popular, which completely abandon the label dependency although their performance is far inferior to fully supervised methods. Zheng et al. <ref type="bibr" coords="3,218.91,703.27,11.23,7.43" target="#b3">[4]</ref> performed segmentation on blood vessels and inpainting blood vessels to normal tissue, then segmented the lesion using representation learning and clustering <ref type="bibr" coords="3,76.49,734.59,14.36,7.43" target="#b24">[25]</ref>. Average NMI score of this method is 0.394 but it is so time-consuming that segmenting one CT case needs over three hours. Some methods <ref type="bibr" coords="3,391.02,66.21,15.38,7.43" target="#b25">[26,</ref><ref type="bibr" coords="3,406.40,66.21,11.54,7.43" target="#b26">27]</ref> applied auto-encoder to learn latent feature from CT images with lesion, and then recovered the images to health, finally obtained the segmentation results by post-processing the images before and after recover. Inspired by these works, we propose a two-branch auto-encoder network, i.e., PCIN for CT image inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image inpainting</head><p>Image inpainting, a technology used to fill in missing regions with plausible alternative contents, is widely used in repairing mask with different shape, target removal, denoising, removing watermark and coloring of old photos <ref type="bibr" coords="3,374.71,177.11,14.36,7.43" target="#b28">[28]</ref>. Image inpainting methods can be grouped into traditional methods and learning-based methods. Traditional patchbased methods <ref type="bibr" coords="3,363.16,197.44,15.38,7.43" target="#b29">[29,</ref><ref type="bibr" coords="3,378.54,197.44,11.54,7.43" target="#b30">30]</ref> fill large holes by searching for the most relevant patches from the complete part of a group of images. Traditional diffusion-based method <ref type="bibr" coords="3,391.13,217.76,15.72,7.43" target="#b31">[31]</ref> smoothly propagates information from the boundary of the missing holes to the interior with variational algorithms. Traditional methods are able to solve the inpainting task with small area, simple structure and texture of the missing region. However, for more complex image inpainting task, learning-based methods have more advantages because of their understanding and perception of high-level semantic information of images. Learning-based methods use generative models such as VAE <ref type="bibr" coords="3,418.28,288.91,15.72,7.43" target="#b32">[32]</ref> and GAN <ref type="bibr" coords="3,470.39,288.91,15.72,7.43" target="#b33">[33]</ref> for inpainting. Context encoder <ref type="bibr" coords="3,353.66,299.07,15.72,7.43" target="#b34">[34]</ref> is the first work to combined auto-encoder network with adversarial training for image inpainting. It generates complete image from semantic features extracted from auto-encoder network and supervises with the pixel-wise reconstruction loss and adversarial loss. The introduction of reconstruction loss makes the inpainting result more realistic but has problems with boundary distortion and local blurring. To solve this problem, Iizuka et al. <ref type="bibr" coords="3,470.05,360.05,15.72,7.43" target="#b35">[35]</ref> added a global discriminator that performed on the whole image region to work with the local discriminator. Local and global structural semantic information drives boundaries of recovered holes to be more continuous.</p><p>In addition, many meaningful improvements have emerged. To address that invalid features of missing holes produce artifacts such as color difference and blur when standard convolution networks are used for image inpainting, Liu et al. <ref type="bibr" coords="3,435.86,431.19,15.72,7.43" target="#b36">[36]</ref> proposed Partial Convolution and Yu et al. <ref type="bibr" coords="3,358.09,441.35,15.72,7.43" target="#b37">[37]</ref> proposed Gate Convolution. Zeng et al. <ref type="bibr" coords="3,524.11,441.35,15.72,7.43" target="#b38">[38]</ref> proposed a pyramid-context encoder to improve the visual and semantic plausibility of inpainting results. In order to improve the inpainting effect on images with large holes, Li et al. <ref type="bibr" coords="3,457.98,471.84,15.72,7.43" target="#b39">[39]</ref> proposed a Progressive Reconstruction of Visual Structure (PRVS) network that progressively reconstructs the structures and the associated visual feature, in which a novel Visual Structure Reconstruction (VSR) layer is proposed to entangle reconstructions of the visual structure and visual feature. Nazeri et al. <ref type="bibr" coords="3,351.33,522.66,15.72,7.43" target="#b40">[40]</ref> proposed EdgeConnect, which imitates sketch art and divides inpainting process into a two-stages: edge generation and image completion. Predicted edge maps from the first stage are passed to the second stage to guide the inpainting process. This method has ability to deal with images with multiple, irregularly shaped missing regions.</p><p>In the field of medical image, inspired by EdgeConnect, Wang et al. <ref type="bibr" coords="3,327.35,583.63,15.72,7.43" target="#b41">[41]</ref> proposed a model based on edge and structure information for CT images inpainting, which utilizes both edge and structure images as priors of the ground truth. It achieves the best performance on several public CT datasets. Inspired from EdgeConnect and PRVS, we used structure (skeleton) information as a prior of ground truth and apply a progressive multiscale strategy to train our proposed CT inpainting network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section first gives the overall workflow of our proposed weakly supervised inpainting-based learning method, then introduces the detection network, next details the structure extraction module (SEM), initial patch generation network (IPGN) and progressive CT inpainting network (PCIN) for inpainting, finally demonstrates the post-processing method for segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall workflow</head><p>The workflow of training our proposed weakly supervised learning method is shown in Fig. <ref type="figure" coords="4,131.24,491.87,3.35,7.43" target="#fig_1">2</ref>  with pseudo healthy image patches ğ‘“ ğ‘–ğ‘ğ‘”ğ‘› (ğ‘§) generated from IPGN, and then its contour and structure information are extracted by SEM and feed into PCIN to get recovered healthy image. Third, the segmentation mask is obtained from the difference image between the input and recovered images using a post-processing algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Detection</head><p>In the first stage of the proposed method, the bounding boxes of lesions are detected by the state-of-the-art object detection network Faster RCNN. Faster RCNN <ref type="bibr" coords="4,403.19,660.38,15.72,7.43" target="#b42">[42]</ref> consists of three main modules: extractor, region proposal network(RPN) and classifier. VGG-16 model with a pre-trained weights from ImageNet is used as a deep features extractor. The obtained feature maps are shared with subsequent modules. RPN is a major innovation of Faster RCNN, it is used to replace the extremely time-consuming selective search algorithm in Fast RCNN to generate region proposals. Specifically, a sliding window (3x3 convolution layer) slides over the shared feature maps to get intermediate feature maps, F. <ref type="bibr" coords="5,45.31,36.65,23.45,5.92">Lu et al.</ref> in which each point is the deep low-dimensional information mapped from the anchor boxes with different sizes and ratios on the input CT image. Then, a 1x1 convolution layer judges whether anchor contains objects and another 1x1 convolution layer predicts the position of anchors. Next, these rough region proposals are mapped back to shared feature maps and cropped out to get a proposal feature maps. Finally, in the classifier, RoI pooling resizes the proposal feature maps to the same size and feeds them into several full connect layers to get the final anchors location and class. In this work, we used a total of 9 anchors including three different scales and aspect ratios for each point based on the distribution of lesion size in our dataset. Table <ref type="table" coords="5,222.11,160.74,4.49,7.43" target="#tab_0">1</ref> shows the initial setting of anchors, where base size is the datum scale of the anchor size. Let ğ‘ denotes base size, ğ‘  and ğ‘Ÿ are certain scale and aspect ratio respectively, thus the anchor size can be calculated as follow:</p><formula xml:id="formula_0" coords="5,37.59,206.89,251.08,33.43">â„ğ‘’ğ‘–ğ‘”â„ğ‘¡ = ğ‘ * ğ‘  * âˆš ğ‘Ÿ (1) ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ = ğ‘ * ğ‘  * âˆš 1âˆ•ğ‘Ÿ<label>(2)</label></formula><p>In this work, we follow the multi-task loss to minimize the objective function, as defined in original Faster RCNN. The total loss îˆ¸ ğ‘‘ğ‘’ğ‘¡ can be divided into two parts: loss of RPN and loss of classifier. We denote it as:</p><formula xml:id="formula_1" coords="5,37.59,297.24,247.59,8.71">îˆ¸ ğ‘‘ğ‘’ğ‘¡ = îˆ¸ ğ‘Ÿğ‘ğ‘› + îˆ¸ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ ğ‘–ğ‘’ğ‘Ÿ (<label>3</label></formula><formula xml:id="formula_2" coords="5,285.18,297.24,3.49,7.43">)</formula><p>where îˆ¸ ğ‘Ÿğ‘ğ‘› and îˆ¸ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ ğ‘–ğ‘’ğ‘Ÿ are the same defined as Faster RCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Holes generation algorithm</head><p>Generating holes similar to the ground truth for training has two advantages: <ref type="bibr" coords="5,83.82,366.20,10.48,7.43" target="#b0">(1)</ref> it is able to improve the generalization ability of the model. ( <ref type="formula" coords="5,83.04,376.66,3.49,7.43" target="#formula_0">2</ref>) it is able to facilitate the application of our proposed inpainting method on datasets with different distribution of lesion. Therefore, the statistics information of the lesion guides us to make the training set îˆ° ğ‘  for the inpainting network.</p><p>Given 472 healthy CT images in îˆµ ğ‘› , we first duplicate them and generate one hole for each CT image, resulting in a set îˆº 1 472 , where subscript represents the total number of images and superscript represents the number of lesion in each image. Next, we calculate the proportion that number of images with two lesions accounted for that of one lesion is about 0.56(according to Fig. <ref type="figure" coords="5,151.76,470.90,13.86,7.43" target="#fig_9">10(a</ref> </p><formula xml:id="formula_3" coords="5,37.59,538.48,251.08,11.54">îˆ° ğ‘  = {îˆº 1 472 , îˆº 2 264 , îˆº 3 282 , îˆº 4 215 , îˆº 5 113 } ğ‘“ ğ‘–ğ‘ğ‘”ğ‘› (4)</formula><p>Considering inpainting task is not sensitive to the aspect ratio of missing regions, the generated holes are squares. The lengths of these square holes are randomly selected from {25, 50, 75, 100, 150, 250} according to Fig. <ref type="figure" coords="5,105.44,587.86,18.01,7.43" target="#fig_9">10(b)</ref>. Moreover, holes are not exceed the minimum boundary rectangle of lungs. Some examples of holes generation algorithm are visualized in Fig. <ref type="figure" coords="5,151.82,608.80,3.36,7.43">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inpainting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Structure extraction module (SEM)</head><p>As shown in Fig. <ref type="figure" coords="5,113.62,661.31,3.36,7.43" target="#fig_1">2</ref>, SEM extracts the structure and contour information from an input CT image îˆ° (ğ‘–)  ğ‘  and feeds them into structural inpainting branch of PCIN. As illustrated in Fig. <ref type="figure" coords="5,208.86,682.25,3.36,7.43">4</ref>, SEM is composed of a series of image processing steps. First, a median filter with kernel size 7 Ã— 7 is applied to smooth the images and further reduce noise, which is mainly tiny blood vessel tissue. Second, skeleton in the CT image is automatic segmented by OTSU <ref type="bibr" coords="5,150.89,724.12,15.71,7.43" target="#b43">[43]</ref> method. Third, we extract all the connected regions using an algorithm based on topological structural analysis <ref type="bibr" coords="5,338.60,56.05,15.72,7.43" target="#b44">[44]</ref> (provided by opencv). Forth, we select the maximum connected region and obtain the final structure information including structure and contour by filling and stroking. It is noted that missing holes of the image are filled with a pixel value of 0 during extraction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Initial patch generation network (IPGN)</head><p>Standard convolution indiscriminately treats all pixels including missing holes as valid pixels that imply semantic information, which has been experimentally shown to result visual artifacts, color differences, etc. Initializing a fixed pixel value for the missing holes has limited improvements. Two effective methods, Partial Convolution and Gate Convolution, take the additional maintenance of a mask to filter out invalid content for standard convolutions. However, as the network deepens, this ability to discriminate whether a pixel is valid or invalid gradually decreases. In this work, IPGN is proposed to generate pseudo healthy image patches to initialize the missing holes. This method reduces the invalidity of pixels in the missing holes and does not increase the burden of inpainting network. As illustrated in Fig. <ref type="figure" coords="5,529.68,244.71,3.36,7.43" target="#fig_5">6</ref>, IPGN uses a 2D random noise of 25 Ã— 25 as input to generate a fake image of 100 Ã— 100. Three types of convolution block are designed for IPGN and the details of PCIN are shown in Fig. <ref type="figure" coords="5,465.39,276.12,3.36,7.43" target="#fig_4">5</ref>. Instance normalization (IN) in CB-A and CB-B can accelerate model convergence and promote style consistency between generated images and real images, so six CB-A are used for feature extraction. Two transposed convolution layers instead of up-sampling layers are used to expand the image size because it works well in image reconstruction from reduced representations, i.e. noise. In order to generate more realistic images, we construct a discriminator to train IPGN utilizing an adversarial loss base on GAN. The discriminator uses one CB-A and five CB-B for feature extraction. Five max pooling layers gradually increase the receptive field of the network while decreasing the size of feature maps. Finally, the classification result is given by the fully connected layer.</p><p>In conclusion, IPGN serves as a generator to learn the data distribution from a random noise, and a discriminator learns to distinguish between the fake output and the real image, hence the adversarial loss for IPGN, îˆ¸ ğ‘–ğ‘ğ‘”ğ‘› , is:</p><formula xml:id="formula_4" coords="5,306.60,449.88,251.08,27.17">îˆ¸ ğ‘–ğ‘ğ‘”ğ‘› = min ğ‘“ ğ‘–ğ‘ğ‘”ğ‘› max ğ· E ğ‘¥âˆ¼î‰„ [ğ‘™ğ‘œğ‘”(ğ·(ğ‘¥))] +E ğ‘§âˆ¼î‰† [ğ‘™ğ‘œğ‘”(1 âˆ’ ğ·(ğ‘“ ğ‘–ğ‘ğ‘”ğ‘› (ğ‘§)))]<label>(5)</label></formula><p>where ğ· represents discriminator, î‰„ and î‰† represent distribution of real images îˆ° ğ‘” and noise ğ‘§, respectively. Some fake images generated by IPGN are shown in Fig. <ref type="figure" coords="5,409.06,504.05,3.36,7.43" target="#fig_6">7</ref>, we observe that the lung parenchyma is realistic but the skeleton is easily distinguished as fake by human eyes. Deformed and disordered skeleton will be further recovered by our proposed inpainting network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Progressive CT inpainting network (PCIN)</head><p>Considering that recovery of skeleton in the missing holes is critical for subsequent segmentation, a separate branch is designed in PCIN to recover skeleton. To our best knowledge, EdgeConnect <ref type="bibr" coords="5,524.08,587.99,15.72,7.43" target="#b40">[40]</ref> first divided the image inpainting process into a two-stages: edge generation and image completion, in which edge generation is solely focused on edges of the missing holes. EdgeConnect enlightens from how artists work, i.e., ''lines first, color next''. However, this method is not applicable to chest CT images that with simple edge information. On this basis, Wang et al. <ref type="bibr" coords="5,392.32,650.82,15.72,7.43" target="#b41">[41]</ref> utilized both edge and structure images as priors of the ground truth for CT images inpainting. However, this structural information retains pulmonary parenchymal features, but results in poor skeleton recovery in his results. To address this problem, we extract pure structure (i.e. skeleton) and contour information from the input CT image using SEM, and then decouple the recovery of skeleton from the overall recovery through a single network which called structure inpainting branch. As shown in Fig. <ref type="figure" coords="5,495.39,724.12,3.36,7.43" target="#fig_7">8</ref>, PCIN fuses the deep structure features with different scales from structure inpainting  ğ‘  initialize by IPGN as input, its corresponding label is îˆµ (ğ‘–) ğ‘› , we extract the ground truth structure image îˆµ ğ‘†(ğ‘–) ğ‘› from it using SEM, the loss function is given by:</p><formula xml:id="formula_5" coords="6,306.60,616.33,247.59,11.07">îˆ¸ ğ‘ğ‘ğ‘–ğ‘› = îˆ¸ ğ‘† ğ‘ğ‘ğ‘–ğ‘› + îˆ¸ ğ¶ ğ‘ğ‘ğ‘–ğ‘› (<label>6</label></formula><formula xml:id="formula_6" coords="6,554.20,617.89,3.49,7.43">)</formula><formula xml:id="formula_7" coords="6,306.60,639.90,251.08,24.36">îˆ¸ ğ‘† ğ‘ğ‘ğ‘–ğ‘› = 5 âˆ‘ ğ‘–=1 ğ¿ ğ‘ ğ‘¡ğ‘’ğ‘ (ğ‘“ ğ‘†(ğ‘–) ğ‘ğ‘ğ‘–ğ‘› , ğ‘“ 5âˆ’ğ‘– ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘§ğ‘’ (îˆµ ğ‘†(ğ‘–) ğ‘› ))<label>(7)</label></formula><formula xml:id="formula_8" coords="6,306.60,670.40,251.08,24.36">îˆ¸ ğ¶ ğ‘ğ‘ğ‘–ğ‘› = 5 âˆ‘ ğ‘–=1 ğ¿ ğ‘ ğ‘¡ğ‘’ğ‘ (ğ‘“ ğ¶(ğ‘–) ğ‘ğ‘ğ‘–ğ‘› , ğ‘“ 5âˆ’ğ‘– ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘§ğ‘’ (îˆµ (ğ‘–) ğ‘› ))<label>(8)</label></formula><p>where ğ‘“ ğ‘¡ ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘§ğ‘’ is a function used to downsample an image ğ‘¡ times with a 2 Ã— 2 stride. ğ¿ ğ‘ ğ‘¡ğ‘’ğ‘ is a binary cross entropy loss function.</p><p>Details of training process are given in Algorithm 1 (see Appendix). In summary, the total loss function of our proposed weakly supervised  inpainting-based learning method, îˆ¸ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ is</p><formula xml:id="formula_9" coords="7,37.59,429.29,251.08,8.71">îˆ¸ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = îˆ¸ ğ‘‘ğ‘’ğ‘¡ + îˆ¸ ğ‘–ğ‘ğ‘”ğ‘› + îˆ¸ ğ‘ğ‘ğ‘–ğ‘› (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Segmentation</head><p>As shown in Fig. <ref type="figure" coords="7,119.80,473.04,3.36,7.43" target="#fig_8">9</ref>, the segmentation stage obtains the lesion mask using blood vessels removal, image difference and a series of post-processing steps. Specifically, Given a CT image îˆ¹ ğ‘–ğ‘› with lesion bounding boxes detected in the first stage and its inpainting result îˆ¹ ğ‘œğ‘¢ğ‘¡ outputs from the second stage, the final segmentation mask îˆ¹ ğ‘ ğ‘’ğ‘” is obtained with the following steps:</p><p>1. A Hessian-based filter <ref type="bibr" coords="7,140.12,542.68,15.72,7.43" target="#b45">[45]</ref> ğ‘“ ğ‘“ ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘– is used to remove major blood vessels that would affect the results to generate îˆ¹ â€² ğ‘–ğ‘› . The specific steps are as follows: First, the ğ‘“ ğ‘“ ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘– is applied to the îˆ¹ ğ‘–ğ‘› to obtain a vessel-enhanced image, denoted as îˆ¹ ğ‘“ .</p><formula xml:id="formula_10" coords="7,61.50,603.06,227.17,8.71">îˆ¹ ğ‘“ = ğ‘“ ğ‘“ ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘– (îˆ¹ ğ‘–ğ‘› )<label>(10)</label></formula><p>Next, the vessel-enhanced image îˆ¹ ğ‘“ is subjected to a thresholding operation to obtain a vessel mask, denoted as îˆ¹ ğ‘¡ğ‘“ (where the vessel regions value is 1, other regions value is 0). Finally, the vessel-removed image îˆ¹ â€² ğ‘–ğ‘› is obtained using the following formula:</p><formula xml:id="formula_11" coords="7,61.50,677.12,227.17,11.01">îˆ¹ â€² ğ‘–ğ‘› = îˆ¹ ğ‘–ğ‘› * (1 âˆ’ îˆ¹ ğ‘¡ğ‘“ )<label>(11)</label></formula><p>2. Get the difference image îˆ¹ ğ‘ ğ‘¢ğ‘ by subtracting îˆ¹ ğ‘œğ‘¢ğ‘¡ from îˆ¹ â€² ğ‘–ğ‘› . This step obtains a rough lesion region.</p><formula xml:id="formula_12" coords="7,61.50,733.09,227.17,11.01">îˆ¹ ğ‘ ğ‘¢ğ‘ = îˆ¹ ğ‘œğ‘¢ğ‘¡ âˆ’ îˆ¹ â€² ğ‘–ğ‘› (12)</formula><p>3. This step starts the post-processing. Taking out all bounding box regions îˆ¹ ğ‘1âˆ¼ğ‘3 ğ‘ ğ‘¢ğ‘ (here assume that the amount is 3) from îˆ¹ ğ‘ ğ‘¢ğ‘ for subsequent processing individually.</p><formula xml:id="formula_13" coords="7,330.52,309.52,227.17,11.25">îˆ¹ ğ‘1âˆ¼ğ‘3 ğ‘ ğ‘¢ğ‘ = ğ¶ğ‘Ÿğ‘œğ‘(îˆ¹ ğ‘ ğ‘¢ğ‘ )<label>(13)</label></formula><p>4. Median filtering with kernel size 7 Ã— 7 ğ‘“ 7Ã—7 ğ‘šğ‘’ğ‘‘ is used to smooth the lesion region, then a binarization with threshold 10 ğ‘“ 10 ğ‘ğ‘–ğ‘› is applied for rough segmentation. Foreground pixels in the segmentation result is 255 and background pixels is 0. Next, a dilation with kernel size 5 Ã— 5 ğ‘“ 5Ã—5</p><p>ğ‘‘ğ‘–ğ‘™ is used to fill tiny gaps, finally an region growing algorithm ğ‘“ ğ‘ ğ‘’ğ‘’ğ‘‘ ğ‘Ÿğ‘’ğ‘” with automatic seed selection is applied to get the accurate lesion segmentation mask. It is noted that regions outside bounding boxes are filled with pixel of 0. Algorithm 2 (see Appendix) details the ğ‘“ ğ‘ ğ‘’ğ‘’ğ‘‘ ğ‘Ÿğ‘’ğ‘” . First, a sliding window with size of 5 Ã— 5 and stride of 2 is used to slide the bounding box region ğ‘… and calculate the mean pixel value, hence a mean value image ğ‘… ğ‘šğ‘’ğ‘ğ‘› is generated. Second, the position of the pixel with largest value in ğ‘… ğ‘šğ‘’ğ‘ğ‘› is calculated as ğ‘ƒ â€² ğ‘–ğ‘›ğ‘–ğ‘¡ (ğ‘¥ â€² , ğ‘¦ â€² ), we then map ğ‘ƒ â€² ğ‘–ğ‘›ğ‘–ğ‘¡ (ğ‘¥, ğ‘¦) back to ğ‘… according to Eq. ( <ref type="formula" coords="7,349.75,476.66,7.48,7.43" target="#formula_14">14</ref>) to get ğ‘ƒ ğ‘–ğ‘›ğ‘–ğ‘¡ (ğ‘¥, ğ‘¦), which is the initial seed pixel. Finally, the segmentation mask ğ‘… ğ‘ ğ‘’ğ‘” is obtained by a standard 8-neighborhood region growing method.</p><formula xml:id="formula_14" coords="7,330.52,511.49,227.17,22.31">ğ‘¥ = 2(ğ‘¥ â€² + 1) ğ‘¦ = 2(ğ‘¦ â€² + 1)<label>(14)</label></formula><p>In summary, we denote this step as:     with lesion. The statistics of the bounding boxes of lesion in the training set is shown in Fig. <ref type="figure" coords="9,109.17,223.25,7.47,7.43" target="#fig_9">10</ref>, we have the following finding: Count. As shown in Fig. <ref type="figure" coords="9,143.81,233.95,7.08,7.43" target="#fig_9">10</ref>(a), the count of the bounding boxes of lesion distributes from 1 to 14 in each CT image and nearly 90% distributes from 1âˆ¼5.</p><formula xml:id="formula_15" coords="7,330.52,556.65,223.43,11.31">îˆ¹ ğ‘1âˆ¼ğ‘3 ğ‘ ğ‘’ğ‘” = ğ‘“ ğ‘ ğ‘’ğ‘’ğ‘‘ ğ‘Ÿğ‘’ğ‘” (ğ‘“ 5Ã—5 ğ‘‘ğ‘–ğ‘™ (ğ‘“ 10 ğ‘ğ‘–ğ‘› (ğ‘“ 7Ã—7 ğ‘šğ‘’ğ‘‘ (îˆ¹ ğ‘1âˆ¼ğ‘3 ğ‘ ğ‘¢ğ‘ ))))<label>(15</label></formula><p>Size. As shown in Fig. <ref type="figure" coords="9,132.67,265.73,7.21,7.43" target="#fig_9">10</ref>(b), we observe that ranges of width and height of the bounding boxes are 5 âˆ¼ 254 and 3 âˆ¼ 373, respectively. Areas are almost under 64000.</p><p>Location. Fig. <ref type="figure" coords="9,103.48,297.52,14.94,7.43" target="#fig_9">10(c</ref>) shows that lesion distributes all over the lung region without obvious preference.</p><p>Aspect ratio. Fig. <ref type="figure" coords="9,116.57,318.76,19.46,7.43" target="#fig_9">10(d)</ref> shows that aspect ratio distribution of the bounding boxes of lesion is centered on 1 and ranges from 0.14 to 4.75.</p><p>Based on these statistics, initial setting of anchors in Faster RCNN are determined as shown Table <ref type="table" coords="9,152.55,350.54,3.36,7.43" target="#tab_0">1</ref>. In addition, Faster RCNN is trained by positive samples in the training set and tested by the whole testing set. PCIN is trained by dataset îˆ° ğ‘  producted from negative samples in the training set using holes generation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental setting 4.2.1. Training details</head><p>For a fair comparison, Faster RCNN, IPGN, PCIN as well as all baselines are trained in the Pytorch framework. Detail parameters for training are listed in Table <ref type="table" coords="9,133.71,457.56,3.36,7.43" target="#tab_4">3</ref>. In addition, all networks except extractor are initialized with Kaiming initialization and the Adam optimizer is used for the optimization of the weights. The division of training datasets for fully supervised segmentation models such as UNet, Attention-UNet, and SegNet in this study is presented in Table <ref type="table" coords="9,265.77,499.72,3.36,7.43" target="#tab_3">2</ref>. The learning rate for all models during training is set to 1e-4. Training and testing are performed alternately, with one round of testing conducted after each round of training. Training is stopped either after 100 rounds or when the loss value becomes smaller than 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Evaluation metrics</head><p>ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›, ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ and average precision (ğ´ğ‘ƒ ) are adopted to evaluate Faster RCNN. For inpainting task, we utilize the widely adopted metrics of mean absolute error (ğ‘€ğ´ğ¸), peak signal to noise ratio (ğ‘ƒ ğ‘†ğ‘ğ‘…) and structural similarity <ref type="bibr" coords="9,163.50,606.26,15.72,7.43" target="#b48">[48]</ref> (ğ‘†ğ‘†ğ¼ğ‘€) for a more comprehensive evaluation. To evaluate the performance of segmentation, Dice similarity coefficient (ğ·ğ‘†ğ¶), intersection over union (ğ¼ğ‘œğ‘ˆ ), sensitivity (ğ‘†ğ¸ğ‘) and specificity (ğ‘†ğ‘ƒ ğ¸) are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines</head><p>we compare our proposed PCIN with the following state-of-the-art methods for image inpainting :</p><p>EC <ref type="bibr" coords="9,57.48,702.79,15.88,7.48" target="#b40">[40]</ref>: EdgeConnect, a two-stage network utilizes edge information as prior for rationally image inpainting.</p><p>CA <ref type="bibr" coords="9,57.68,724.03,16.28,7.48" target="#b34">[34]</ref>: Context encoder, the first work to combine auto-encoder network with adversarial training for image inpainting task. PC <ref type="bibr" coords="9,326.53,212.72,15.94,7.48" target="#b36">[36]</ref>: Partial convolution, a convolution layer which only convolutes the valid region outside the missing part.</p><p>GC <ref type="bibr" coords="9,326.77,233.60,16.41,7.48" target="#b37">[37]</ref>: Gate convolution, an improved method of partial convolution, which provides a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers.</p><p>Furthermore, three excellent medical image segmentation networks (i. MiniSeg <ref type="bibr" coords="9,346.76,390.18,16.11,7.48" target="#b18">[19]</ref>: An extremely lightweight network for efficient COVID-19 segmentation. A novel multi-scale Attentive Hierarchical Spatial Pyramid (AHSP) module is adopted to ensure the accuracy under the constraint of the extremely minimum network size.</p><p>InfNet <ref type="bibr" coords="9,341.30,431.94,15.16,7.48" target="#b23">[24]</ref>: An semi-supervised segmentation framework based on a randomly selected propagation strategy, only a few labeled images are needed when training. A fully supervised version is also provided and we implement it in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Detection results</head><p>In this work, bounding boxes of false positive (FP) has little influence on subsequent segmentation when compared that of false negative (FN). Hence, the ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ of Faster RCNN should be as high as possible. However, as illustrated in Fig. <ref type="figure" coords="9,421.39,536.29,22.02,7.43" target="#fig_12">11 (b)</ref>, ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ is not sensitive to IoU threshold before 0.8. To make trade-off between ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› and ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™, the IoU threshold is set to 0.8, and the corresponding ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›, ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ and ğ´ğ‘ƒ are 0.62, 0.70 and 0.66, respectively. Fig. <ref type="figure" coords="9,489.05,567.60,8.97,7.43" target="#fig_13">12</ref> shows detection results of three lung CT images, in which the IoU of all bounding boxes and the labeled ground truth are greater than 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inpainting results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Quantitative results</head><p>We quantitatively compare our proposed PCIN with four representative state-of-the-art image inpainting methods: EC, CA, PC and GC. Table <ref type="table" coords="9,345.15,661.52,4.49,7.43" target="#tab_5">4</ref> shows the overall performance comparison. It is obvious that PCIN has the best performance. The performance of existing four methods differs greatly, among which CA is the best one with ğ‘€ğ´ğ¸ of 49.89, ğ‘ƒ ğ‘†ğ‘ğ‘… of 12.01 and ğ‘†ğ‘†ğ¼ğ‘€ of 0.1784. Relatively good results of CA verify that encoder-decoder structure and reconstructionadversarial joint loss are useful for image inpainting. PCIN adopts the encoder-decoder structure but gives up using the adversarial loss because it is difficult to converge when training. In summary, PCIN     <ref type="table" coords="10,236.41,639.45,3.36,7.43" target="#tab_5">4</ref>, it is obvious that experimental results are consistent with our suppose. Our method is at least 3.6 points of ğ‘ƒ ğ‘†ğ‘ğ‘… higher than methods initialized with constants. Additionally, an interesting phenomenon is observed that the smaller the initial value, the better the inpainting performance. Table <ref type="table" coords="10,71.02,692.39,4.49,7.43" target="#tab_6">5</ref> shows the performance comparison on different holes ratios. We observe that existing methods show better inpainting performance for small-area missing holes but the performance decays severely as the missing area increases. Instead, Our method is not only superior in performance, but also stable on different holes ratios. Empirically, skeleton part dominates the overall inpainting effect for large holes. The prior knowledge provided by the structure inpainting branch ensures a stable inpainting performance of ğ‘ƒ ğ¶ğ¼ğ‘.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Qualitative results</head><p>Fig. <ref type="figure" coords="10,336.39,597.87,8.97,7.43" target="#fig_15">13</ref> illustrates the visual results of PCIN and existing four methods (EC, CA, PC and GC). Results show that our method generates most reasonable and realistic content. Results of the existing four methods are easily identified as fake due to the disparity errors and visual artifacts such as color leakage, blurring discontinuity and the partial occlusion. EC and GC incorrectly recover the lung parenchyma of missing holes to skeleton and the results of GC are more blur. On the contrary, PC recovers the lung parenchyma of missing holes to skeleton, with an appearance of peeling. This is because the mask of partial convolution network gradually disappears in the deeper layer, which forces the network to make ambiguity decision when training. CA achieves relatively good results but exists serious checkerboard artifacts caused by deconvolution layers. In summary, PCIN applies structure inpainting branch for skeleton recovery, and then fuses deep   structure features to comprehensive inpainting branch for a reasonable complete result generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Segmentation results</head><p>To compare the lesion segmentation performance with fully supervised methods, we consider three state-of-the-art medical image segmentation networks (i.e., UNet, Attention-UNet and SegNet) and five recently proposed COVID-19 lesion segmentation networks (i.e., JCS, COVID-rate, AnamNet, MiniSeg and InfNet). The number of parameters, FLOPs, and baseline are shown in Table <ref type="table" coords="11,182.60,578.81,3.36,7.43" target="#tab_7">6</ref>. We can clearly see that the numbers of parameters and FLOPs of IPGN and PCIN are at a moderate level compared to these fully supervised models. Quantitative results are shown in Table <ref type="table" coords="11,112.33,609.97,3.36,7.43" target="#tab_8">7</ref>, our method outperforms SegNet and UNet in terms of ğ·ğ‘†ğ¶, and is comparable to Attention-UNet. The ğ·ğ‘†ğ¶ of our method is only 7.54% lower than that of the best performing network InfNet. The ğ‘†ğ¸ğ‘ is defined as the proportion of true-positive (TP) pixels in the predicted image, which is considered more important compared to other metrics in medical image segmentation. Our threestage method is a process from coarse to fine, covering more possible lesion regions in the detection stage, hence results an excellent ğ‘†ğ¸ğ‘ below InfNet and COVID-Rate. GrabCut is a semi-automatic image segmentation algorithm based on graph cut. It requires the user to input a bounding box as the segmentation target area, enabling the segmentation of the object from the background. Our method outperforms GrabCut in terms of ğ·ğ‘†ğ¶, ğ¼ğ‘œğ‘ˆ and ğ‘†ğ¸ğ‘. The ğ‘†ğ¸ğ‘ metric surpasses GrabCut by 39.34%. We have achieved such a significant improvement compared to GrabCut because GrabCut does not effectively separate the lesions from the background.</p><p>To analyze the impact of inpainting quality on segmentation results, we compared our PCIN method with six different inpainting algorithms: EC, CA, GC, PC, Fast Marching and Navier-Stokes. Among these, Fast Marching and Navier-Stokes are lightweight diffusion-based inpainting methods. As shown in Table <ref type="table" coords="11,408.61,389.70,3.36,7.43" target="#tab_9">8</ref>, when the inpainting results of PCIN are directly subtracted from the original image to obtain the segmentation results without using post-processing, we achieved the best Dice score of 74.28%, which is 1.75% higher than the second-ranking EC method. The IoU metric also yielded the best result at 68.22%, surpassing the EC method by 2.31%. After applying the post-processing, our method shows a 4.06% improvement in ğ·ğ‘†ğ¶, a 2.6% improvement in ğ¼ğ‘œğ‘ˆ , and a 5.06% improvement in ğ‘†ğ¸ğ‘. These results demonstrate the effectiveness of the post-processing method in enhancing the segmentation outcomes. Among the two lightweight restoration methods, Navier-Stokes exhibits the best performance, with the smallest difference compared to the deep learning restoration method PC. It shows a difference of 0.71% in ğ·ğ‘†ğ¶, 0.67% in ğ¼ğ‘œğ‘ˆ , and 1.2% in ğ‘†ğ¸ğ‘.</p><p>Fig. <ref type="figure" coords="11,336.05,525.58,8.97,7.43" target="#fig_16">14</ref> illustrates the visual comparison of lesion segmentation results between five recently proposed COVID-19 lesion segmentation networks and our method. As can be seen, results of InfNet and JCS are closest to the ground truth. MiniSeg performs also excellent but has many false positive pixels in Fig. <ref type="figure" coords="11,424.14,567.39,18.08,7.43" target="#fig_16">14(d)</ref>. The segmentation performance of COVID-rate and AnamNet for lesion with small area (Fig. <ref type="figure" coords="11,539.11,577.85,14.86,7.43;11,306.60,588.30,3.24,7.43" target="#fig_16">14(ac</ref>)) is better than that for lesion with big area (Fig. <ref type="figure" coords="11,495.72,588.30,20.76,7.43" target="#fig_16">14(d-e</ref>)). Instead, our method is relatively stable and with less false positive pixels than COVID-rate and AnamNet. In addition, results of Figs. <ref type="figure" coords="11,501.10,609.20,15.22,7.43" target="#fig_16">14(a</ref>) and 14(b) show that our method outperforms most other methods. In summary, our proposed weakly supervised method achieves a comparable segmentation performance to fully supervised methods using cost effective bounding box labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a weakly supervised inpainting-based method for lung CT images segmentation, in which only cost effective bounding box labels are required for training. Our method is a coarse to fine process consisting of three stages: detection, inpainting and segmentation. Detected lesion regions from detection stage are first   cropped out of the input image, then the missing holes are recovered to normal regions by progressive CT inpainting network (PCIN), finally the accurate segmentation mask is obtained by a post-process method from the difference image of input and recovered images. In order to remove the visual artifacts resulted from the invalid features of missing holes, we also propose a patch generation network (IPGN) for holes initialization with generated pseudo healthy image patches. Experiments on public COVID-19 dataset verify the effectiveness of our methods. Moreover, our method achieves comparable segmentation performance to some models trained using fully supervised labels. It strikes a good balance between label marking cost and segmentation accuracy.</p><p>Although our proposed method is comparable to fully supervised methods, we also have some challenges. First, our method is not onestep, which contains three learnable models (Faster RCNN, IPGN and PCIN), so the training process of the model is relatively complex. Second, segmentation performance is prone to be affected by the preceding two stages. During the detection phase, false positive results have minimal impact on segmentation performance because the regions falsely identified as positive are restored to healthy regions during the hole-filling process. However, false negative results can significantly decrease segmentation performance as the missing lesion regions do not participate in subsequent stages, and this impact is irreversible. In the inpainting phase, the proposed inpainting method in this paper does not generate non-existing tissues, thus it has minimal effect on segmentation performance. Third, the edge of segmentation results from our method is rough and has a few gaps. In future work, we will try to design a network to perform both detection and inpainting tasks, and the performance of post-processing will be further optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,37.59,193.06,519.60,6.85;2,37.59,202.51,520.10,5.94;2,37.59,211.08,34.96,5.94;2,147.66,55.79,300.00,125.71"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Input CT images with missing rectangle holes (drawn in red). (b) Structure of CT images recovered from structure inpainting branch of PCIN. (c) Complete results 1 from comprehensive inpainting branch of PCIN. (d) Ground truth. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="2,147.66,55.79,300.00,125.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,37.59,420.00,520.10,6.93;4,37.59,428.59,520.10,6.91;4,37.59,437.16,442.75,6.91;4,103.53,55.79,388.32,351.79"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Workflow of our proposed weakly supervised inpainting-based learning method for COVID-19 lesion segmentation. In the (a) training period, positive samples îˆ° ğ‘ are used for the training of Faster RCNN. Dataset for PCIN is made by generating rectangle holes on negative images îˆµ ğ‘› based on real information of the bounding boxes of lesion from îˆ® ğ‘ . Image patches îˆ° ğ‘” cropped from îˆµ ğ‘› are utilized to train IPGN. (b) Inference period consists of three stages: detection, inpainting and segmentation.</figDesc><graphic coords="4,103.53,55.79,388.32,351.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.59,491.87,154.08,7.61;4,37.59,502.42,251.08,8.71;4,37.59,512.96,251.08,8.71;4,37.59,523.51,251.08,7.61;4,37.59,534.05,251.09,8.71;4,37.59,544.60,251.08,8.71;4,37.59,555.14,251.08,8.71;4,37.59,565.69,251.08,7.43;4,37.59,576.24,251.08,8.71;4,37.59,586.78,251.09,7.43;4,37.59,597.33,251.08,8.71;4,37.59,607.85,251.08,8.73;4,37.59,618.42,251.08,7.43;4,37.59,628.94,251.09,7.48;4,37.59,639.51,251.09,7.43;4,37.59,650.06,251.08,7.43;4,37.59,660.58,251.08,7.48;4,37.59,671.15,251.08,8.71;4,37.59,681.67,251.09,7.48;4,37.59,692.24,109.25,7.43;4,49.54,702.95,239.13,7.43;4,37.59,713.50,251.09,7.43;4,37.59,724.05,251.08,7.43;4,37.59,734.59,251.08,7.43"><head></head><label></label><figDesc>(a). Let îˆ° is the training set used in this work, which consists of positive (COVID-19) samples îˆ° ğ‘ and negative (healthy) samples îˆ° ğ‘› . Denote îˆ° ğ‘ = îˆµ ğ‘ âˆª îˆ® ğ‘ , îˆ° ğ‘› = îˆµ ğ‘› âˆª îˆ® ğ‘› , where îˆµ and îˆ® represent CT images and their corresponding bounding box labels. îˆ° ğ‘ serves two purposes. First, it is used for training the detection network Faster RCNN ğ‘“ ğ‘‘ğ‘’ğ‘¡ . Second, îˆ® ğ‘ in îˆ° ğ‘ is used to make the training set îˆ° ğ‘  for the inpainting network. We generate rectangle holes based on the statistical information of the bounding boxes of lesion from îˆ® ğ‘ and crop them out of each images in îˆµ ğ‘› , in the way that, the location, number and size of rectangle holes in each image of îˆ° ğ‘  are very similar to the ground truth. In addition, image patches cropped from îˆµ ğ‘› compose a training set îˆ° ğ‘” for training IPGN. IPGN generates pseudo healthy image patches to replace the cropped holes that would introduce artifacts. PCIN first recovers the missing skeleton using structure inpainting branch, then fuses deep structure features to comprehensive inpainting branch for a more reasonable complete result generation. Specially, when training the PCIN, given a training image from îˆ° ğ‘  as the input of comprehensive inpainting branch, we extract the contour and structure information using SEM as the input of structure inpainting branch. As shown in Fig. 2(b). Inferencing period proceeds in three steps: detection, inpainting and segmentation. First, the test CT image is input into Faster RCNN to detect the bounding boxes of lesion. Second, the detected bounding boxes are cropped out of the input image and filled</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,37.59,242.03,255.65,6.82;6,290.03,246.80,6.69,3.91;6,297.23,241.98,13.68,6.86;6,307.71,246.80,6.69,3.91;6,317.50,242.03,22.35,6.82;6,336.65,246.80,6.69,3.91;6,346.44,242.76,211.24,5.94;6,37.59,251.33,520.10,5.94;6,37.59,259.89,80.69,5.94;6,153.64,55.79,288.00,174.53"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Visualization of holes generation algorithm. (aâˆ¼c) shows some images from îˆº 2 264 , îˆº 3 282 and îˆº 4 215 respectively. Green color represents minimum boundary rectangle of left lung and right lung. Red color represents generated holes and their center points. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="6,153.64,55.79,288.00,174.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,37.59,588.12,251.09,5.99;6,37.59,596.71,251.08,5.94;6,37.59,605.28,251.09,5.94;6,37.59,613.84,129.62,5.94;6,45.88,448.66,234.52,127.04"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Convolution blocks used in our proposed IPGN and PCIN. Instance normalization (IN) can accelerate model convergence and promote style consistency between generated images and real images. Transposed convolution layer works well in image reconstruction from reduced representations.</figDesc><graphic coords="6,45.88,448.66,234.52,127.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,37.59,234.09,520.10,5.99;7,37.59,242.68,97.45,5.94;7,87.64,55.79,420.09,165.88"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The structure of initial patch generation network. IPGN generates fake images of 100 Ã— 100 from 2D random noises of 25 Ã— 25. A discriminator is designed and utilized an adversarial loss to train IPGN.</figDesc><graphic coords="7,87.64,55.79,420.09,165.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,42.74,379.61,240.77,5.99;7,38.93,262.57,248.44,104.61"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison between (a) real images and (b) IPGN generated fake images.</figDesc><graphic coords="7,38.93,262.57,248.44,104.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,37.59,255.96,520.10,5.99;8,37.59,264.54,449.15,5.94;8,87.64,58.28,420.18,185.25"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The structure of progressive CT inpainting network. PCIN decouples the recovery of skeleton from the overall recovery through structure inpainting branch. Deep structure features with different scales extracted from structure inpainting branch are fused to comprehensive inpainting branch for a reasonable result generation.</figDesc><graphic coords="8,87.64,58.28,420.18,185.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="8,110.47,432.35,374.33,5.99;8,87.64,287.52,420.08,132.40"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Details of segmentation, which consists of blood vessels removal, image difference and a series of post-processing steps.</figDesc><graphic coords="8,87.64,287.52,420.08,132.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="8,37.59,724.55,520.10,5.99;8,37.59,733.13,57.86,5.94;8,139.74,455.37,315.84,256.75"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Statistics of the bounding boxes of lesion in the training set. (a) Count distribution. (b) Size distribution. (c) Distribution of lesion location. (d) Aspect ratio distribution. Zoom in for details.</figDesc><graphic coords="8,139.74,455.37,315.84,256.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="9,314.61,275.35,243.08,7.48;9,306.60,285.79,251.08,7.48;9,306.60,296.23,251.08,7.48;9,306.60,306.70,133.82,7.43;9,318.56,317.11,239.13,7.48;9,306.60,327.57,251.09,7.43;9,318.56,337.99,239.13,7.48;9,306.60,348.45,251.08,7.43;9,306.60,358.89,204.85,7.43;9,318.56,369.30,239.13,7.48;9,306.60,379.77,250.44,7.43"><head></head><label></label><figDesc>e., UNet, Attention-UNet and SegNet) and six recently proposed COVID-19 lesion segmentation networks methods (i.e., JCS, COVID-Rate, AnamNet, MiniSeg and InfNet) are used for the comparison of the lesion segmentation performance. JCS[49]: A novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID-19 chest CT diagnosis. COVID-rate[50]: An U-Net like network for COVID-19 lesion segmentation. It contains a Context Perception Boosting (CPB) Module to capture the long-range dependencies of the global image. AnamNet[18]: An anamorphic depth embedding-based lightweight network for anomalies segmentation from COVID-19 chest CT images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="10,37.59,36.65,31.17,5.92"><head>F</head><label></label><figDesc>.Lu et al.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="10,57.82,162.10,479.63,5.99;10,124.98,55.79,345.36,93.89"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Performance of Faster RCNN. (a) and (b) are Precision and Recall under different IoU thresholds, respectively. (c) shows the Precision-Recall (PR) curves.</figDesc><graphic coords="10,124.98,55.79,345.36,93.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="10,37.59,316.76,520.10,5.99;10,37.59,325.34,285.41,5.94;10,119.15,199.07,357.12,105.26"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Visual results of lesion detection using Faster RCNN. The bounding boxes of lesion and their prediction possibilities are shown in red color. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="10,119.15,199.07,357.12,105.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="11,37.59,36.65,31.17,5.92"><head>F</head><label></label><figDesc>.Lu et al.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="11,90.33,280.03,414.62,5.99;11,87.63,55.79,420.21,211.82"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Visual results of PCIN and four existing methods. GT represents ground truth. PCIN generates most reasonable and realistic content.</figDesc><graphic coords="11,87.63,55.79,420.21,211.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="12,37.59,407.04,520.10,5.99;12,37.59,415.63,420.33,5.94;12,87.63,195.64,420.21,198.98"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Visual comparison of lesion segmentation results. GT represents ground truth. The green, blue, and red regions refer to true positive, false negative and false positive pixels, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="12,87.63,195.64,420.21,198.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,315.12,459.38,229.51,62.88"><head>Table 1</head><label>1</label><figDesc>Initial setting of anchors.</figDesc><table /><note>Base size 8 Scale (4, 16, 32) Aspect ratio (0.5, 1, 1.5) Anchor size (26, 39), (32 32), (45 22), (105 157), (128 128), (width, height) (181 90), (209 314), (256 256), (362 181)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,306.61,558.21,251.09,183.81"><head></head><label></label><figDesc>) Dataset from zenodo contains 20 COVID-19 CT scans with lung and lesion segmentation labels annotated by two radiologists, and then verified by an experienced radiologist. Dataset from MosMedData contains CT images of 1110 patients (254 normal and 856 COVID-19 cases), we used 50 normal cases and 50 COVID-19 cases respectively. CT images with lung area less than 4.5% percent of the total image are filtered out because they have small valid area and contains no lesion. We then split the dataset into a training set and testing set, and summarize it in Table2. It is noted that ''Positive'' means CT images</figDesc><table coords="7,306.61,586.09,251.08,71.66"><row><cell>4. Experiment</cell></row><row><cell>4.1. Dataset</cell></row><row><cell>4.1.1. Public COVID-19 segmentation dataset</cell></row><row><cell>Two public dataset [46,47] from zenodo and MosMedData are used</cell></row><row><cell>in this work.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,59.11,55.79,203.50,54.25"><head>Table 2</head><label>2</label><figDesc>Details of the public dataset.</figDesc><table coords="9,63.64,75.08,198.97,34.96"><row><cell></cell><cell>Positive</cell><cell>Negative</cell><cell>Total</cell></row><row><cell>Train</cell><cell>2508</cell><cell>1720</cell><cell>4228</cell></row><row><cell>Test</cell><cell>496</cell><cell>158</cell><cell>654</cell></row><row><cell>Total</cell><cell>3004</cell><cell>1878</cell><cell>4882</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,59.11,128.98,203.51,54.31"><head>Table 3</head><label>3</label><figDesc>Detail parameters of training.</figDesc><table coords="9,63.64,148.32,198.97,34.96"><row><cell>Network</cell><cell>Batch size</cell><cell>Epochs</cell><cell>Learning rate</cell></row><row><cell>Faster RCNN</cell><cell>6</cell><cell>100</cell><cell>1eâˆ’5</cell></row><row><cell>IPGN</cell><cell>20</cell><cell>100</cell><cell>2eâˆ’4</cell></row><row><cell>PCIN</cell><cell>2</cell><cell>100</cell><cell>2e-3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,328.13,55.79,203.01,130.42"><head>Table 4</head><label>4</label><figDesc>Comparison of inpainting performance with different methods.</figDesc><table coords="9,328.13,74.47,203.01,111.74"><row><cell>Methods</cell><cell>ğ‘€ğ´ğ¸ âˆ’</cell><cell>ğ‘ƒ ğ‘†ğ‘ğ‘… +</cell><cell>ğ‘†ğ‘†ğ¼ğ‘€ +</cell></row><row><cell>EC</cell><cell>81.08</cell><cell>7.33</cell><cell>0.1224</cell></row><row><cell>CA</cell><cell>49.89</cell><cell>12.01</cell><cell>0.1784</cell></row><row><cell>PC</cell><cell>63.97</cell><cell>10.27</cell><cell>0.1101</cell></row><row><cell>GC</cell><cell>93.12</cell><cell>7.49</cell><cell>0.1263</cell></row><row><cell>PCIN(0)</cell><cell>21.23</cell><cell>17.87</cell><cell>0.3042</cell></row><row><cell>PCIN(127)</cell><cell>23.63</cell><cell>17.58</cell><cell>0.3021</cell></row><row><cell>PCIN(255)</cell><cell>24.83</cell><cell>17.04</cell><cell>0.2889</cell></row><row><cell>PCIN(ğ‘šğ‘’ğ‘ğ‘›)</cell><cell>23.47</cell><cell>17.61</cell><cell>0.3032</cell></row><row><cell>PCIN(ğ‘“ ğ‘–ğ‘ğ‘”ğ‘› )</cell><cell>14.93</cell><cell>21.47</cell><cell>0.4182</cell></row><row><cell cols="3">PCIN( * ):  *  represents initial value of missing holes.</cell><cell></cell></row><row><cell cols="3">ğ‘šğ‘’ğ‘ğ‘›: The mean pixel value of the training set.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,37.59,362.27,437.55,284.61"><head>Table 5</head><label>5</label><figDesc>Comparison of image inpainting performance for different ratios of missing area.</figDesc><table coords="10,37.59,381.62,437.55,265.26"><row><cell></cell><cell>Ratio</cell><cell>0âˆ¼2%</cell><cell>2âˆ¼4%</cell><cell>4âˆ¼6%</cell><cell>6âˆ¼8%</cell><cell>8%âˆ¼10%</cell><cell>&gt;10%</cell></row><row><cell></cell><cell>EC</cell><cell>32.51</cell><cell>48.05</cell><cell>63.74</cell><cell>76.55</cell><cell>85.24</cell><cell>95.17</cell></row><row><cell></cell><cell>CA</cell><cell>19.98</cell><cell>39.21</cell><cell>61.35</cell><cell>81.46</cell><cell>103.81</cell><cell>122.43</cell></row><row><cell>ğ‘€ğ´ğ¸ âˆ’</cell><cell>PC</cell><cell>17.89</cell><cell>30.03</cell><cell>44.11</cell><cell>62.36</cell><cell>95.19</cell><cell>109.43</cell></row><row><cell></cell><cell>GC</cell><cell>28.43</cell><cell>39.67</cell><cell>55.01</cell><cell>68.47</cell><cell>79.19</cell><cell>96.81</cell></row><row><cell></cell><cell>PCIN</cell><cell>7.27</cell><cell>7.22</cell><cell>7.02</cell><cell>7.65</cell><cell>6.63</cell><cell>6.19</cell></row><row><cell></cell><cell>EC</cell><cell>13.93</cell><cell>10.53</cell><cell>9.08</cell><cell>8.17</cell><cell>7.18</cell><cell>6.6</cell></row><row><cell></cell><cell>CA</cell><cell>17.05</cell><cell>10.84</cell><cell>8.02</cell><cell>6.41</cell><cell>4.88</cell><cell>4.4</cell></row><row><cell>ğ‘ƒ ğ‘†ğ‘ğ‘… +</cell><cell>PC</cell><cell>18.31</cell><cell>13.37</cell><cell>10.67</cell><cell>7.93</cell><cell>5.29</cell><cell>4.31</cell></row><row><cell></cell><cell>GC</cell><cell>15.14</cell><cell>12.05</cell><cell>9.42</cell><cell>8.38</cell><cell>7.21</cell><cell>6.29</cell></row><row><cell></cell><cell>PCIN</cell><cell>27.46</cell><cell>24.1</cell><cell>26.94</cell><cell>26.11</cell><cell>25.54</cell><cell>23.39</cell></row><row><cell></cell><cell>EC</cell><cell>0.6712</cell><cell>0.6043</cell><cell>0.4931</cell><cell>0.3727</cell><cell>0.2643</cell><cell>0.1282</cell></row><row><cell></cell><cell>CA</cell><cell>0.5686</cell><cell>0.4305</cell><cell>0.3423</cell><cell>0.2633</cell><cell>0.2226</cell><cell>0.0418</cell></row><row><cell>ğ‘†ğ‘†ğ¼ğ‘€ +</cell><cell>PC</cell><cell>0.6914</cell><cell>0.6185</cell><cell>0.4902</cell><cell>0.3731</cell><cell>0.2959</cell><cell>0.1126</cell></row><row><cell></cell><cell>GC</cell><cell>0.3551</cell><cell>0.4041</cell><cell>0.3039</cell><cell>0.2966</cell><cell>0.3666</cell><cell>0.2769</cell></row><row><cell></cell><cell>PCIN</cell><cell>0.6435</cell><cell>0.6378</cell><cell>0.6528</cell><cell>0.6419</cell><cell>0.6845</cell><cell>0.6634</cell></row><row><cell cols="4">has an improvement of leaps and bounds on performance. PC and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">GC both update convolution to cope with invalid values of missing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">holes. However, experimental results imply that such methods are</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ineffective on lung CT images. Instead, our innovative approach solves</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the problem well from the source, i.e., IPGN generates pseudo healthy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">image patches for initialization that reduces the invalidity of pixels</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of the missing holes. To further confirm the effectiveness of IPGN,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ablation study is performed. As shown in Table 4, different constants</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">are utilized to initialize holes pixels, including 0, 127, 255 and mean</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">pixel value of the training set (121). As shown in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,37.59,316.28,251.09,129.48"><head>Table 6</head><label>6</label><figDesc>Comparison of the deep learning models used in this segmentation process in terms of number of training parameters and FLOPs.</figDesc><table coords="11,42.12,344.19,242.02,101.57"><row><cell>Methods</cell><cell>Baseline</cell><cell>#Param</cell><cell>FLOPs</cell></row><row><cell>UNet</cell><cell>-</cell><cell>31.04M</cell><cell>421.49G</cell></row><row><cell>Attention-UNet</cell><cell>UNet</cell><cell>34.87M</cell><cell>513.79G</cell></row><row><cell>SegNet</cell><cell>VGG</cell><cell>29.44M</cell><cell>422.07M</cell></row><row><cell>JCS</cell><cell>VGG16&amp;Res2net101</cell><cell>117.72M</cell><cell>484.14G</cell></row><row><cell>COVID-rate</cell><cell>-</cell><cell>14.38M</cell><cell>297.92G</cell></row><row><cell>AnamNet</cell><cell>-</cell><cell>4.47M</cell><cell>196.12G</cell></row><row><cell>MiniSeg</cell><cell>-</cell><cell>0.8M</cell><cell>1.03G</cell></row><row><cell>InfNet</cell><cell>-</cell><cell>31.07M</cell><cell>57.56G</cell></row><row><cell>IPGN</cell><cell>GAN</cell><cell>1.61M</cell><cell>16.17G</cell></row><row><cell>PCIN</cell><cell>VGG16</cell><cell>25.46M</cell><cell>236.94G</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,154.61,55.79,281.52,120.94"><head>Table 7</head><label>7</label><figDesc>Comparison of lesion segmentation performance.</figDesc><table coords="12,159.14,75.14,276.99,101.59"><row><cell>Label</cell><cell>Methods</cell><cell>ğ·ğ‘†ğ¶(%)</cell><cell>ğ¼ğ‘œğ‘ˆ (%)</cell><cell>ğ‘†ğ¸ğ‘(%)</cell><cell>ğ‘†ğ‘ƒ ğ¸(%)</cell></row><row><cell></cell><cell>UNet</cell><cell>77.23</cell><cell>70.41</cell><cell>77.05</cell><cell>99.91</cell></row><row><cell>Full</cell><cell>Attention-UNet</cell><cell>80.22</cell><cell>73.81</cell><cell>79.18</cell><cell>99.91</cell></row><row><cell></cell><cell>SegNet</cell><cell>76.82</cell><cell>69.95</cell><cell>74.88</cell><cell>99.91</cell></row><row><cell></cell><cell>JCS</cell><cell>82.33</cell><cell>76.38</cell><cell>81.95</cell><cell>99.90</cell></row><row><cell></cell><cell>COVID-rate</cell><cell>81.96</cell><cell>75.68</cell><cell>84.67</cell><cell>99.85</cell></row><row><cell>Full</cell><cell>AnamNet</cell><cell>81.62</cell><cell>75.08</cell><cell>79.95</cell><cell>99.92</cell></row><row><cell></cell><cell>MiniSeg</cell><cell>81.05</cell><cell>74.72</cell><cell>80.53</cell><cell>99.90</cell></row><row><cell></cell><cell>InfNet</cell><cell>85.88</cell><cell>79.93</cell><cell>87.01</cell><cell>99.89</cell></row><row><cell>Bounding Box</cell><cell>GrabCut</cell><cell>43.87</cell><cell>43.42</cell><cell>43.61</cell><cell>99.83</cell></row><row><cell></cell><cell>Ours</cell><cell>78.34</cell><cell>70.82</cell><cell>82.95</cell><cell>99.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,37.59,451.83,246.55,103.80"><head>Table 8</head><label>8</label><figDesc>Comparison of segmentation performance for inpainting networks.</figDesc><table coords="12,42.12,471.18,242.02,84.46"><row><cell>Methods</cell><cell>ğ·ğ‘†ğ¶(%)</cell><cell>ğ¼ğ‘œğ‘ˆ (%)</cell><cell>ğ‘†ğ¸ğ‘(%)</cell><cell>ğ‘†ğ‘ƒ ğ¸(%)</cell></row><row><cell>Fast Marching</cell><cell>67.20</cell><cell>58.82</cell><cell>68.29</cell><cell>99.51</cell></row><row><cell>Navier-Stokes</cell><cell>67.39</cell><cell>58.98</cell><cell>68.05</cell><cell>99.55</cell></row><row><cell>EC</cell><cell>72.53</cell><cell>65.91</cell><cell>77.79</cell><cell>99.46</cell></row><row><cell>CA</cell><cell>69.89</cell><cell>61.68</cell><cell>72.08</cell><cell>99.49</cell></row><row><cell>PC</cell><cell>68.10</cell><cell>59.65</cell><cell>69.25</cell><cell>99.46</cell></row><row><cell>GC</cell><cell>72.29</cell><cell>64.23</cell><cell>77.08</cell><cell>99.42</cell></row><row><cell>PCIN</cell><cell>74.28</cell><cell>68.22</cell><cell>73.01</cell><cell>99.50</cell></row><row><cell>PCIN(Post-Processing)</cell><cell>78.34</cell><cell>70.82</cell><cell>78.07</cell><cell>99.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="13,37.59,36.65,238.93,556.72"><head>:</head><label></label><figDesc>F.Lu  et al. Training process of PCIN. Training images initialize by IPGN ğ‘“ ğ‘–ğ‘ğ‘”ğ‘› â† îˆ° ğ‘  Labels â† îˆµ ğ‘› Output: Trained PCIN ğ‘“ ğ‘ğ‘ğ‘–ğ‘› . Initialize the parameters of the ğ‘“ ğ‘ğ‘ğ‘–ğ‘› ; 2 freeze the parameters of the ğ‘“ ğ¶ ğ‘ğ‘ğ‘–ğ‘› ;</figDesc><table coords="13,37.59,57.49,141.69,257.77"><row><cell cols="3">Algorithm 1Input: 3 repeat</cell><cell></cell></row><row><cell>4</cell><cell cols="4">foreach image îˆ° (ğ‘–) ğ‘  in îˆ° ğ‘  do</cell></row><row><cell>5</cell><cell>îˆµ ğ‘†(ğ‘–) ğ‘›</cell><cell cols="2">, îˆµ ğ¶(ğ‘–) ğ‘›</cell><cell>â† ğ‘†ğ¸ğ‘€(îˆµ (ğ‘–) ğ‘› )</cell></row><row><cell>6</cell><cell>îˆ° ğ‘†(ğ‘–) ğ‘ </cell><cell cols="2">, îˆ° ğ¶(ğ‘–) ğ‘ </cell><cell>â† ğ‘†ğ¸ğ‘€(îˆ° (ğ‘–) ğ‘  )</cell></row><row><cell>7</cell><cell cols="2">ğ‘“ ğ‘†(1âˆ¼5) ğ‘ğ‘ğ‘–ğ‘›</cell><cell cols="2">â† ğ‘“ ğ‘† ğ‘ğ‘ğ‘–ğ‘› (îˆ° ğ‘†(ğ‘–) ğ‘ </cell><cell>, îˆ° ğ¶(ğ‘–) ğ‘  )</cell></row><row><cell>8</cell><cell cols="2">ğ‘“ ğ¶(1âˆ¼5) ğ‘ğ‘ğ‘–ğ‘›</cell><cell cols="2">â† ğ‘“ ğ¶ ğ‘ğ‘ğ‘–ğ‘› (îˆ° (ğ‘–) ğ‘  )</cell></row><row><cell>9</cell><cell cols="4">calculate îˆ¸ ğ‘ğ‘ğ‘–ğ‘› by Eqs. (6)-(8)</cell></row><row><cell>10</cell><cell cols="4">optimize ğ‘“ ğ‘† ğ‘ğ‘ğ‘–ğ‘›</cell></row><row><cell cols="5">11 until ğ‘“ ğ‘† ğ‘ğ‘ğ‘–ğ‘› converges</cell></row><row><cell cols="5">12 unfreeze the parameters of the ğ‘“ ğ¶ ğ‘ğ‘ğ‘–ğ‘› ;</cell></row><row><cell cols="2">13 repeat</cell><cell></cell><cell></cell></row><row><cell>14</cell><cell cols="4">foreach image îˆ° (ğ‘–) ğ‘  in îˆ° ğ‘  do</cell></row><row><cell>15</cell><cell>îˆµ ğ‘†(ğ‘–) ğ‘›</cell><cell cols="2">, îˆµ ğ¶(ğ‘–) ğ‘›</cell><cell>â† ğ‘†ğ¸ğ‘€(îˆµ (ğ‘–) ğ‘› )</cell></row><row><cell>16</cell><cell>îˆ° ğ‘†(ğ‘–) ğ‘ </cell><cell cols="2">, îˆ° ğ¶(ğ‘–) ğ‘ </cell><cell>â†ğ‘†ğ¸ğ‘€(îˆ° (ğ‘–) ğ‘  )</cell></row><row><cell>17</cell><cell cols="2">ğ‘“ ğ‘†(1âˆ¼5) ğ‘ğ‘ğ‘–ğ‘›</cell><cell cols="2">â† ğ‘“ ğ‘† ğ‘ğ‘ğ‘–ğ‘› (îˆ° ğ‘†(ğ‘–) ğ‘ </cell><cell>, îˆ° ğ¶(ğ‘–) ğ‘  )</cell></row><row><cell>18</cell><cell cols="2">ğ‘“ ğ¶(1âˆ¼5) ğ‘ğ‘ğ‘–ğ‘›</cell><cell cols="2">â† ğ‘“ ğ¶ ğ‘ğ‘ğ‘–ğ‘› (îˆ° (ğ‘–) ğ‘  )</cell></row></table><note>1 19 calculate îˆ¸ ğ‘ğ‘ğ‘–ğ‘› by Eqs. (6)-(8) 20 optimize ğ‘“ ğ‘ğ‘ğ‘–ğ‘› 21 until ğ‘“ ğ‘ğ‘ğ‘–ğ‘› converges 22 return Trained ğ‘“ ğ‘ğ‘ğ‘–ğ‘› Algorithm 2: Automatic seeded region growing algorithm. Input: A binary image ğ‘…. Output: Segmentation mask ğ‘… ğ‘ ğ‘’ğ‘” . 1 # Select seeds by sliding a window with size of 5Ã—5, and stride of 2. (ğ‘ ğ‘¡ğ‘’ğ‘ 2 âˆ¼ 4) 2 ğ‘… ğ‘šğ‘’ğ‘ğ‘› = ğ´ğ‘£ğ‘”ğ‘ğ‘œğ‘œğ‘™(ğ‘…, ğ‘˜ = 5, ğ‘  = 2) 3 ğ‘ƒ â€² ğ‘–ğ‘›ğ‘–ğ‘¡ (ğ‘¥ â€² , ğ‘¦ â€² ) = max(ğ‘… ğ‘šğ‘’ğ‘ğ‘› ) 4 ğ‘ƒ ğ‘–ğ‘›ğ‘–ğ‘¡ (ğ‘¥, ğ‘¦) = ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”(ğ‘ƒ â€² ğ‘–ğ‘›ğ‘–ğ‘¡ (ğ‘¥ â€² , ğ‘¦ â€² )) 5 Initialize a vector of seeds ğ‘£ğ‘’ğ‘{ğ‘ƒ ğ‘–ğ‘›ğ‘–ğ‘¡ (ğ‘¥, ğ‘¦)} 6 Initialize a 2D array ğ´ to record whether a pixel have been accessed 7 Initialize the ğ‘… ğ‘ ğ‘’ğ‘” with pixel value of 0 8 while ğ‘£ğ‘’ğ‘ is not empty do 9 Pop a seed ğ‘ƒ (ğ‘¥, ğ‘¦) from ğ‘£ğ‘’ğ‘ 10 ğ´(ğ‘¥, ğ‘¦) = ğ‘¡ğ‘Ÿğ‘¢ğ‘’, ğ‘… ğ‘ ğ‘’ğ‘” (ğ‘¥, ğ‘¦) = 255 11 Get the neighboring 8 pixels of ğ‘ƒ (ğ‘¥, ğ‘¦) and denote it as ğ‘ƒ ğ‘› 12 foreach point ğ‘(ğ‘–, ğ‘—) in ğ‘ƒ ğ‘› do 13 if ğ´(ğ‘–, ğ‘—) == ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’ and ğ‘…(ğ‘–, ğ‘—) == 255 then 14 ğ´(ğ‘–, ğ‘—) = ğ‘¡ğ‘Ÿğ‘¢ğ‘’, ğ‘… ğ‘ ğ‘’ğ‘” (ğ‘–, ğ‘—) = 255 15 push ğ‘(ğ‘¥, ğ‘¦) into ğ‘£ğ‘’ğ‘</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="13,38.81,597.38,51.53,42.11"><head>16 end 17 end 18 end 19 return ğ‘… ğ‘ ğ‘’ğ‘”</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability</head><p>Data will be made available on request.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. Algorithm</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,56.55,698.29,232.13,5.94;13,56.55,706.86,232.13,5.94;13,56.55,715.43,45.14,5.94" xml:id="b0">
	<analytic>
		<title level="a" type="main">Role of intelligent computing in COVID-19 prognosis: A state-of-the-art review</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Swapnarekha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page">109947</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Swapnarekha, H.S. Behera, J. Nayak, B. Naik, Role of intelligent computing in COVID-19 prognosis: A state-of-the-art review, Chaos Solitons Fractals 138 (2020) 109947.</note>
</biblStruct>

<biblStruct coords="13,56.55,727.15,232.12,5.94;13,56.55,735.71,115.67,5.94" xml:id="b1">
	<analytic>
		<title level="a" type="main">A woman with fever and cough: coronavirus disease</title>
		<author>
			<persName coords=""><forename type="first">Y.-Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intern. Emerg. Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1553" to="1554" />
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y.-Q. Lu, A woman with fever and cough: coronavirus disease 2019, Intern. Emerg. Med. 15 (8) (2020) 1553-1554.</note>
</biblStruct>

<biblStruct coords="13,325.56,57.17,232.13,5.94;13,325.56,65.74,232.13,5.94;13,325.56,74.31,205.64,5.94" xml:id="b2">
	<analytic>
		<title level="a" type="main">Longitudinal assessment of COVID-19 using a deep learning-based quantitative CT pipeline: illustration of two cases</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology: Cardiothorac. Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e200082</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Cao, Z. Xu, J. Feng, C. Jin, X. Han, H. Wu, H. Shi, Longitudinal assessment of COVID-19 using a deep learning-based quantitative CT pipeline: illustration of two cases, Radiology: Cardiothorac. Imaging 2 (2) (2020) e200082.</note>
</biblStruct>

<biblStruct coords="13,325.56,83.30,232.13,5.94;13,325.56,91.87,232.12,5.94;13,325.56,100.44,232.13,5.94;13,325.56,109.01,232.13,5.94;13,325.56,117.57,152.85,5.94" xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of COVID-19 infected lung clinical CT volumes using image inpainting and representation learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Otake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Akashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Takabatake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">11596</biblScope>
			<biblScope unit="page">115963F</biblScope>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
	<note type="raw_reference">T. Zheng, M. Oda, C. Wang, T. Moriya, Y. Hayashi, Y. Otake, M. Hashimoto, T. Akashi, M. Mori, H. Takabatake, et al., Unsupervised segmentation of COVID- 19 infected lung clinical CT volumes using image inpainting and representation learning, in: Medical Imaging 2021: Image Processing, Vol. 11596, International Society for Optics and Photonics, 2021, p. 115963F.</note>
</biblStruct>

<biblStruct coords="13,325.56,126.57,232.13,5.94;13,325.56,135.14,232.12,5.94;13,325.56,143.70,108.40,5.94" xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Chen, L. Wu, J. Zhang, L. Zhang, H. Yu, Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography, Sci. Rep. 10 (1) (2020).</note>
</biblStruct>

<biblStruct coords="13,325.56,152.70,232.13,5.94;13,325.56,161.27,232.12,5.94;13,325.56,169.84,181.56,5.94" xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for discriminative localization, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition, (CVPR), IEEE, 2016, pp. 2921-2929.</note>
</biblStruct>

<biblStruct coords="13,325.56,178.83,232.12,5.94;13,325.56,187.40,225.01,5.94" xml:id="b6">
	<analytic>
		<title level="a" type="main">GrabCut&apos;&apos; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Rother, V. Kolmogorov, A. Blake, &apos;&apos;GrabCut&apos;&apos; interactive foreground extraction using iterated graph cuts, ACM Trans. Graph. (TOG) 23 (3) (2004) 309-314.</note>
</biblStruct>

<biblStruct coords="13,325.56,196.39,232.13,5.94;13,325.56,204.96,232.13,5.94;13,325.56,213.53,232.12,5.94;13,325.56,222.10,147.67,5.94" xml:id="b7">
	<analytic>
		<title level="a" type="main">Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="683" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Rajchl, M.C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai, M. Damodaram, M.A. Rutherford, J.V. Hajnal, B. Kainz, et al., Deepcut: Object seg- mentation from bounding box annotations using convolutional neural networks, IEEE Trans. Med. Imaging 36 (2) (2016) 674-683.</note>
</biblStruct>

<biblStruct coords="13,325.56,231.09,232.13,5.94;13,325.56,239.66,232.12,5.94;13,325.56,248.23,189.46,5.94" xml:id="b8">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Lin, J. Dai, J. Jia, K. He, J. Sun, Scribblesup: Scribble-supervised convolutional networks for semantic segmentation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3159-3167.</note>
</biblStruct>

<biblStruct coords="13,325.56,257.22,232.12,5.94;13,325.56,265.79,232.12,5.94;13,325.56,274.36,232.12,5.94;13,325.56,282.93,26.51,5.94" xml:id="b9">
	<analytic>
		<title level="a" type="main">Scribble-based hierarchical weakly supervised learning for brain tumor segmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="175" to="183" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Ji, Y. Shen, C. Ma, M. Gao, Scribble-based hierarchical weakly supervised learning for brain tumor segmentation, in: International Conference on Medi- cal Image Computing and Computer-Assisted Intervention, Springer, 2019, pp. 175-183.</note>
</biblStruct>

<biblStruct coords="13,325.56,291.92,232.13,5.94;13,325.56,300.49,232.13,5.94;13,325.56,309.06,107.39,5.94" xml:id="b10">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Bearman, O. Russakovsky, V. Ferrari, L. Fei-Fei, What&apos;s the point: Semantic segmentation with point supervision, in: European Conference on Computer Vision, Springer, 2016, pp. 549-565.</note>
</biblStruct>

<biblStruct coords="13,325.56,318.05,232.13,5.94;13,325.56,326.62,232.12,5.94;13,325.56,335.19,20.94,5.94" xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, Adv. Neural Inf. Process. Syst. 28 (2015).</note>
</biblStruct>

<biblStruct coords="13,325.56,344.18,232.13,5.94;13,325.56,352.75,232.12,5.94;13,325.56,361.32,226.65,5.94" xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note type="raw_reference">O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedical image segmentation, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2015, pp. 234-241.</note>
</biblStruct>

<biblStruct coords="13,325.56,370.31,232.13,5.94;13,325.56,378.88,232.12,5.94;13,325.56,387.45,232.13,5.94;13,325.56,396.02,46.11,5.94" xml:id="b13">
	<analytic>
		<title level="a" type="main">Unet++: A nested unet architecture for medical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Zhou, M.M. Rahman Siddiquee, N. Tajbakhsh, J. Liang, Unet++: A nested u- net architecture for medical image segmentation, in: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Springer, 2018, pp. 3-11.</note>
</biblStruct>

<biblStruct coords="13,325.56,405.01,232.13,5.94;13,325.56,413.58,232.12,5.94;13,325.56,422.15,172.95,5.94" xml:id="b14">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural networks for volumetric medical image segmentation, in: 2016 Fourth International Conference on 3D Vision, (3DV), IEEE, 2016, pp. 565-571.</note>
</biblStruct>

<biblStruct coords="13,325.56,431.14,232.13,5.94;13,325.56,439.71,232.12,5.94;13,325.56,448.28,167.97,5.94" xml:id="b15">
	<monogr>
		<title level="m" type="main">COVID-19 chest CT image segmentation-a deep convolutional neural network solution</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10987</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Q. Yan, B. Wang, D. Gong, C. Luo, W. Zhao, J. Shen, Q. Shi, S. Jin, L. Zhang, Z. You, COVID-19 chest CT image segmentation-a deep convolutional neural network solution, 2020, arXiv preprint arXiv:2004.10987.</note>
</biblStruct>

<biblStruct coords="13,325.56,457.27,232.13,5.94;13,325.56,465.84,232.13,5.94;13,325.56,474.41,227.26,5.94" xml:id="b16">
	<analytic>
		<title level="a" type="main">A noise-robust framework for automatic segmentation of COVID-19 pneumonia lesions from CT images</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2653" to="2663" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng, K. Li, N. Huang, S. Zhang, A noise-robust framework for automatic segmentation of COVID-19 pneumonia lesions from CT images, IEEE Trans. Med. Imaging 39 (8) (2020) 2653-2663.</note>
</biblStruct>

<biblStruct coords="13,325.56,483.40,232.13,5.94;13,325.56,491.97,232.13,5.94;13,325.56,500.54,232.13,5.94;13,325.56,509.11,124.67,5.94" xml:id="b17">
	<analytic>
		<title level="a" type="main">Anam-net: Anamorphic depth embedding-based lightweight CNN for segmentation of anomalies in COVID-19 chest CT images</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Paluru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dayal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">B</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakinis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">R</forename><surname>Cenkeramaddi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Yalavarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="932" to="946" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Paluru, A. Dayal, H.B. Jenssen, T. Sakinis, L.R. Cenkeramaddi, J. Prakash, P.K. Yalavarthy, Anam-net: Anamorphic depth embedding-based lightweight CNN for segmentation of anomalies in COVID-19 chest CT images, IEEE Trans. Neural Netw. Learn. Syst. 32 (3) (2021) 932-946.</note>
</biblStruct>

<biblStruct coords="13,325.56,518.10,232.13,5.94;13,325.56,526.67,232.12,5.94;13,325.56,535.24,138.12,5.94" xml:id="b18">
	<analytic>
		<title level="a" type="main">MiniSeg: An extremely minimum network for efficient COVID-19 segmentation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4846" to="4854" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Qiu, Y. Liu, S. Li, J. Xu, MiniSeg: An extremely minimum network for efficient COVID-19 segmentation, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35, (6) 2021, pp. 4846-4854.</note>
</biblStruct>

<biblStruct coords="13,325.56,544.23,232.13,5.94;13,325.56,552.80,226.25,5.94" xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation by iterative affinity learning</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1736" to="1749" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">X. Wang, S. Liu, H. Ma, M.-H. Yang, Weakly-supervised semantic segmentation by iterative affinity learning, Int. J. Comput. Vis. 128 (6) (2020) 1736-1749.</note>
</biblStruct>

<biblStruct coords="13,325.56,561.80,232.13,5.94;13,325.56,570.36,232.12,5.94;13,325.56,578.93,134.97,5.94" xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised segmentation of covid19 infection with scribble annotation on ct images</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">108341</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">X. Liu, Q. Yuan, Y. Gao, K. He, S. Wang, X. Tang, J. Tang, D. Shen, Weakly supervised segmentation of covid19 infection with scribble annotation on ct images, Pattern Recognit. 122 (2022) 108341.</note>
</biblStruct>

<biblStruct coords="13,325.56,587.93,232.13,5.94;13,325.56,596.49,232.12,5.94;13,325.56,605.06,232.13,5.94;13,325.56,613.63,228.75,5.94" xml:id="b21">
	<analytic>
		<title level="a" type="main">A weakly supervised consistency-based learning method for covid-19 segmentation in ct images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Manas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kurzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
				<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2453" to="2462" />
		</imprint>
	</monogr>
	<note type="raw_reference">I. Laradji, P. Rodriguez, O. Manas, K. Lensink, M. Law, L. Kurzman, W. Parker, D. Vazquez, D. Nowrouzezahrai, A weakly supervised consistency-based learning method for covid-19 segmentation in ct images, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 2453-2462.</note>
</biblStruct>

<biblStruct coords="13,325.56,622.62,232.12,5.94;13,325.56,631.19,232.13,5.94;13,325.56,639.76,86.65,5.94" xml:id="b22">
	<analytic>
		<title level="a" type="main">RCTE: A reliable and consistent temporalensembling framework for semi-supervised segmentation of COVID-19 lesions</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abdel-Basset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hawash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">578</biblScope>
			<biblScope unit="page" from="559" to="573" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Ding, M. Abdel-Basset, H. Hawash, RCTE: A reliable and consistent temporal- ensembling framework for semi-supervised segmentation of COVID-19 lesions, Inf. Sci. 578 (2021) 559-573.</note>
</biblStruct>

<biblStruct coords="13,325.56,648.75,232.13,5.94;13,325.56,657.32,232.12,5.94;13,325.56,665.89,118.79,5.94" xml:id="b23">
	<analytic>
		<title level="a" type="main">Inf-net: Automatic covid-19 lung infection segmentation from ct images</title>
		<author>
			<persName coords=""><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2626" to="2637" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, L. Shao, Inf-net: Automatic covid-19 lung infection segmentation from ct images, IEEE Trans. Med. Imaging 39 (8) (2020) 2626-2637.</note>
</biblStruct>

<biblStruct coords="13,325.56,674.89,232.13,5.94;13,325.56,683.45,176.79,5.94" xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning feature representations with k-means</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Coates, A.Y. Ng, Learning feature representations with k-means, in: Neural Networks: Tricks of the Trade, Springer, 2012, pp. 561-580.</note>
</biblStruct>

<biblStruct coords="13,325.56,692.45,232.13,5.94;13,325.56,701.02,194.04,5.94" xml:id="b25">
	<analytic>
		<title level="a" type="main">Label-free segmentation of COVID-19 lesions in lung CT</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2808" to="2819" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Q. Yao, L. Xiao, P. Liu, S.K. Zhou, Label-free segmentation of COVID-19 lesions in lung CT, IEEE Trans. Med. Imaging 40 (10) (2021) 2808-2819.</note>
</biblStruct>

<biblStruct coords="13,325.56,710.01,232.13,5.94;13,325.56,718.58,232.12,5.94;13,325.56,727.15,232.13,5.94;13,325.56,735.71,164.52,5.94" xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised detection of pulmonary opacities for computer-aided diagnosis of Covid-19 on CT images</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2021</date>
			<biblScope unit="page" from="9007" to="9014" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Xu, X. Cao, Y. Wang, Y.-W. Chen, X. Ye, L. Lin, W. Zhu, C. Chen, F. Xu, Y. Zhou, et al., Unsupervised detection of pulmonary opacities for computer-aided diagnosis of Covid-19 on CT images, in: 2020 25th International Conference on Pattern Recognition (ICPR), IEEE, 2021, pp. 9007-9014.</note>
</biblStruct>

<biblStruct coords="14,37.59,36.65,31.17,5.92" xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">F. Lu et al.</note>
</biblStruct>

<biblStruct coords="14,56.55,57.17,232.13,5.94;14,56.55,65.74,104.64,5.94" xml:id="b28">
	<analytic>
		<title level="a" type="main">Image inpainting based on deep learning: A review</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Displays</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">102028</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Qin, Q. Zeng, Y. Zong, F. Xu, Image inpainting based on deep learning: A review, Displays 69 (2021) 102028.</note>
</biblStruct>

<biblStruct coords="14,56.55,74.31,232.13,5.94;14,56.55,82.88,232.13,5.94;14,56.55,91.44,51.65,5.94" xml:id="b29">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Barnes, E. Shechtman, A. Finkelstein, D.B. Goldman, PatchMatch: A random- ized correspondence algorithm for structural image editing, ACM Trans. Graph. 28 (3) (2009) 24.</note>
</biblStruct>

<biblStruct coords="14,56.55,100.01,232.13,5.94;14,56.55,108.58,157.33,5.94" xml:id="b30">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="861" to="868" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Sun, L. Yuan, J. Jia, H.-Y. Shum, Image completion with structure propagation, in: ACM SIGGRAPH 2005 Papers, 2005, pp. 861-868.</note>
</biblStruct>

<biblStruct coords="14,56.55,117.15,232.13,5.94;14,56.55,125.72,201.59,5.94" xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenom</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L.I. Rudin, S. Osher, E. Fatemi, Nonlinear total variation based noise removal algorithms, Physica D: Nonlinear Phenom. 60 (1-4) (1992) 259-268.</note>
</biblStruct>

<biblStruct coords="14,56.55,134.28,232.12,5.94;14,56.55,142.85,217.86,5.94" xml:id="b32">
	<analytic>
		<title level="a" type="main">Information constraints on auto-encoding variational bayes</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Regier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yosef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Lopez, J. Regier, M.I. Jordan, N. Yosef, Information constraints on auto-encoding variational bayes, Adv. Neural Inf. Process. Syst. 31 (2018).</note>
</biblStruct>

<biblStruct coords="14,56.55,151.42,232.13,5.94;14,56.55,159.99,232.12,5.94;14,56.55,168.55,193.31,5.94" xml:id="b33">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets in: advances in neural information processing systems (NIPS)</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative adversarial nets in: advances in neural information processing systems (NIPS), Springer, New York, 2014.</note>
</biblStruct>

<biblStruct coords="14,56.55,177.12,232.12,5.94;14,56.55,185.69,232.12,5.94;14,56.55,194.26,189.46,5.94" xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, A.A. Efros, Context encoders: Feature learning by inpainting, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2536-2544.</note>
</biblStruct>

<biblStruct coords="14,56.55,202.83,232.13,5.94;14,56.55,211.39,173.38,5.94" xml:id="b35">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Iizuka, E. Simo-Serra, H. Ishikawa, Globally and locally consistent image completion, ACM Trans. Graph. (TOG) 36 (4) (2017) 1-14.</note>
</biblStruct>

<biblStruct coords="14,56.55,219.96,232.13,5.94;14,56.55,228.53,232.12,5.94;14,56.55,237.10,175.19,5.94" xml:id="b36">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Liu, F.A. Reda, K.J. Shih, T.-C. Wang, A. Tao, B. Catanzaro, Image inpainting for irregular holes using partial convolutions, in: Proceedings of the European Conference on Computer Vision, (ECCV), 2018, pp. 85-100.</note>
</biblStruct>

<biblStruct coords="14,56.55,245.67,232.12,5.94;14,56.55,254.23,232.13,5.94;14,56.55,262.80,160.77,5.94" xml:id="b37">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, T.S. Huang, Free-form image inpaint- ing with gated convolution, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 4471-4480.</note>
</biblStruct>

<biblStruct coords="14,56.55,271.37,232.12,5.94;14,56.55,279.94,232.12,5.94;14,56.55,288.50,189.46,5.94" xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning pyramid-context encoder network for high-quality image inpainting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Zeng, J. Fu, H. Chao, B. Guo, Learning pyramid-context encoder network for high-quality image inpainting, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 1486-1494.</note>
</biblStruct>

<biblStruct coords="14,56.55,297.07,232.12,5.94;14,56.55,305.64,232.12,5.94;14,56.55,314.21,126.49,5.94" xml:id="b39">
	<analytic>
		<title level="a" type="main">Progressive reconstruction of visual structure for image inpainting</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5962" to="5971" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Li, F. He, L. Zhang, B. Du, D. Tao, Progressive reconstruction of visual structure for image inpainting, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 5962-5971.</note>
</biblStruct>

<biblStruct coords="14,56.55,322.78,232.13,5.94;14,56.55,331.34,232.13,5.94;14,56.55,339.91,232.13,5.94" xml:id="b40">
	<analytic>
		<title level="a" type="main">EdgeConnect: Structure guided image inpainting using edge prediction</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3265" to="3274" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Nazeri, E. Ng, T. Joseph, F. Qureshi, M. Ebrahimi, EdgeConnect: Structure guided image inpainting using edge prediction, in: 2019 IEEE/CVF International Conference on Computer Vision Workshop, (ICCVW), IEEE, 2019, pp. 3265-3274.</note>
</biblStruct>

<biblStruct coords="14,56.55,348.48,232.12,5.94;14,56.55,357.05,148.08,5.94" xml:id="b41">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image inpainting with edge and structure priors</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page">110027</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Q. Wang, Y. Chen, N. Zhang, Y. Gu, Medical image inpainting with edge and structure priors, Measurement 185 (2021) 110027.</note>
</biblStruct>

<biblStruct coords="14,56.55,365.62,232.13,5.94;14,56.55,374.18,116.76,5.94" xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Girshick, Fast r-cnn, in: Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1440-1448.</note>
</biblStruct>

<biblStruct coords="14,56.55,382.75,232.12,5.94;14,56.55,391.32,113.97,5.94" xml:id="b43">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Otsu, A threshold selection method from gray-level histograms, IEEE Trans. Syst. Man Cybern. 9 (1) (1979) 62-66.</note>
</biblStruct>

<biblStruct coords="14,56.55,399.89,232.13,5.94;14,56.55,408.45,223.15,5.94" xml:id="b44">
	<analytic>
		<title level="a" type="main">Topological structural analysis of digitized binary images by border following</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis., Graph. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="46" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Suzuki, et al., Topological structural analysis of digitized binary images by border following, Comput. Vis., Graph. Image Process. 30 (1) (1985) 32-46.</note>
</biblStruct>

<biblStruct coords="14,56.55,417.02,232.13,5.94;14,56.55,425.59,232.12,5.94;14,56.55,434.16,192.86,5.94" xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiscale vessel enhancement filtering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
	<note type="raw_reference">A.F. Frangi, W.J. Niessen, K.L. Vincken, M.A. Viergever, Multiscale vessel enhancement filtering, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 1998, pp. 130-137.</note>
</biblStruct>

<biblStruct coords="14,56.55,442.73,232.12,5.94;14,56.55,451.29,232.12,5.94;14,56.55,459.86,59.50,5.94" xml:id="b46">
	<monogr>
		<title level="m" type="main">COVID-19 CT lung and infection segmentation dataset</title>
		<author>
			<persName coords=""><forename type="first">Gecheng</forename><surname>Majun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liuxin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Zenodo</pubPlace>
		</imprint>
	</monogr>
	<note>data set</note>
	<note type="raw_reference">Majun, Gecheng, Y. Wang, X. An, J. Gao, Z. Yu, M. Zhang, Liuxin, X. Deng, S. Cao, COVID-19 CT lung and infection segmentation dataset (verson 1.0) [data set], Zenodo (2020).</note>
</biblStruct>

<biblStruct coords="14,56.55,468.43,232.13,5.94;14,56.55,477.00,232.13,5.94;14,56.55,485.57,232.13,5.94;14,56.55,494.13,58.43,5.94" xml:id="b47">
	<analytic>
		<title level="a" type="main">MosMedData: data set of 1110 chest CT scans performed during the COVID-19 epidemic</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Andreychenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">A</forename><surname>Blokhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">B</forename><surname>Gelezhe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Gonchar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">Y</forename><surname>Chernina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">A</forename><surname>Gombolevskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Diagn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S.P. Morozov, A.E. Andreychenko, I.A. Blokhin, P.B. Gelezhe, A.P. Gonchar, A.E. Nikolaev, N.A. Pavlov, V.Y. Chernina, V.A. Gombolevskiy, MosMedData: data set of 1110 chest CT scans performed during the COVID-19 epidemic, Digital Diagn. 1 (1) (2020) 49-59.</note>
</biblStruct>

<biblStruct coords="14,56.55,502.70,232.13,5.94;14,56.55,511.27,232.13,5.94;14,56.55,519.84,48.32,5.94" xml:id="b48">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Wang, A.C. Bovik, H.R. Sheikh, E.P. Simoncelli, Image quality assessment: from error visibility to structural similarity, IEEE Trans. Image Process. 13 (4) (2004) 600-612.</note>
</biblStruct>

<biblStruct coords="14,56.55,528.41,232.12,5.94;14,56.55,536.97,232.12,5.94;14,56.55,545.54,146.56,5.94" xml:id="b49">
	<analytic>
		<title level="a" type="main">Jcs: An explainable covid-19 diagnosis system by joint classification and segmentation</title>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R.-G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3113" to="3126" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y.-H. Wu, S.-H. Gao, J. Mei, J. Xu, D.-P. Fan, R.-G. Zhang, M.-M. Cheng, Jcs: An explainable covid-19 diagnosis system by joint classification and segmentation, IEEE Trans. Image Process. 30 (2021) 3113-3126.</note>
</biblStruct>

<biblStruct coords="14,325.56,57.17,232.12,5.94;14,325.56,65.74,232.12,5.94;14,325.56,74.31,232.12,5.94;14,325.56,82.88,15.74,5.94" xml:id="b50">
	<analytic>
		<title level="a" type="main">COVID-rate: an automated framework for segmentation of COVID-19 lesions from chest CT images</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Enshaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oikonomou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Rafiee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Heidarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Naderkhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Enshaei, A. Oikonomou, M.J. Rafiee, P. Afshar, S. Heidarian, A. Mohammadi, K.N. Plataniotis, F. Naderkhani, COVID-rate: an automated framework for seg- mentation of COVID-19 lesions from chest CT images, Sci. Rep. 12 (1) (2022) 1-18.</note>
</biblStruct>

<biblStruct coords="14,306.60,114.27,251.08,5.99;14,306.60,122.86,251.08,5.94;14,306.60,131.43,251.09,5.94;14,306.60,139.99,251.09,5.94;14,306.60,148.56,32.63,5.94" xml:id="b51">
	<monogr>
		<title level="m" type="main">She is currently an assistant professor in Shanghai University of Electric Power. Her current research focuses on medical image processing, machine learning, pattern recognition and image quality assessment</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Shanghai, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>from Shanghai Jiaotong University</orgName>
		</respStmt>
	</monogr>
	<note>Fangfang Lu received the Ph.D. degree in control theory and control engineering</note>
	<note type="raw_reference">Fangfang Lu received the Ph.D. degree in control theory and control engineering from Shanghai Jiaotong University, Shanghai, China, in 2013. She is currently an assistant professor in Shanghai University of Electric Power. Her current research focuses on medical image processing, machine learning, pattern recognition and image quality assessment.</note>
</biblStruct>

<biblStruct coords="14,306.60,171.08,251.08,5.99;14,306.60,179.67,251.08,5.94;14,306.60,188.24,251.08,5.94;14,306.60,196.81,251.08,5.94;14,306.60,205.37,136.81,5.94" xml:id="b52">
	<monogr>
		<title level="m" type="main">He is Currently working toward the M.S. degree in artificial intelligence and big data From Shanghai University of Electric Power. His current research focuses on medical image processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Nanjing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Electronical Information Science and Technology from Nanjing XiaoZhuang University</orgName>
		</respStmt>
	</monogr>
	<note>Zhihao Zhang received the B.E. degree in. deep learning and defect detection</note>
	<note type="raw_reference">Zhihao Zhang received the B.E. degree in Electronical Information Science and Technology from Nanjing XiaoZhuang University, Nanjing, China in 2018. He is Currently working toward the M.S. degree in artificial intelligence and big data From Shanghai University of Electric Power. His current research focuses on medical image processing, deep learning and defect detection.</note>
</biblStruct>

<biblStruct coords="14,306.60,229.25,251.08,5.99;14,306.60,237.84,251.08,5.94;14,306.60,246.41,251.09,5.94;14,306.60,254.98,251.09,5.94;14,306.60,263.54,25.05,5.94" xml:id="b53">
	<monogr>
		<title level="m" type="main">He is currently working toward the M.S. degree in electronical information computer science and technology from Shanghai University of Electric Power. His current research focuses on medical image processing and deep learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Zhangzhou, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xiamen University Tan Kah Kee College</orgName>
		</respStmt>
	</monogr>
	<note>Tianxiang Liu received the B.E. degree in</note>
	<note type="raw_reference">Tianxiang Liu received the B.E. degree in Xiamen University Tan Kah Kee College, Zhangzhou, China, in 2020. He is currently working toward the M.S. degree in electronical information computer science and technology from Shanghai University of Electric Power. His current research focuses on medical image processing and deep learning.</note>
</biblStruct>

<biblStruct coords="14,306.60,287.42,251.08,5.99;14,306.60,296.01,251.08,5.94;14,306.60,304.58,251.08,5.94;14,306.60,313.15,251.08,5.94;14,306.60,337.03,251.08,5.99;14,306.60,345.61,251.09,5.94;14,306.60,354.18,251.08,5.94;14,306.60,362.75,251.08,5.94;14,306.60,371.32,93.63,5.94" xml:id="b54">
	<analytic>
		<title level="a" type="main">He is currently working toward the M.S. degree in artificial intelligence and big data from Shanghai University of Electric Power. His current research focuses on deep learning and image processing</title>
	</analytic>
	<monogr>
		<title level="m">Chi Tang received the B.E. degree in mechanical and electronic information process from Nantong Institute of technology</title>
				<meeting><address><addrLine>Nantong, China; Xuzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology from Xuzhou University of Technology</orgName>
		</respStmt>
	</monogr>
	<note>He is currently working toward the M.S. degree in artificial intelligence and big data from Shanghai University of Electric Power. His current research focuses on rock reconstruction. deep learning and generative adversarial networks</note>
	<note type="raw_reference">Chi Tang received the B.E. degree in mechanical and electronic information process from Nantong Institute of technology, Nantong, China, in 2020. He is currently working toward the M.S. degree in artificial intelligence and big data from Shanghai University of Electric Power. His current research focuses on deep learning and image processing. Hualin Bai received the B.E. degree in Computer Science and Technology from Xuzhou University of Technology, Xuzhou, China, in 2019. He is currently working toward the M.S. degree in artificial intelligence and big data from Shanghai University of Electric Power. His current research focuses on rock reconstruction, deep learning and generative adversarial networks.</note>
</biblStruct>

<biblStruct coords="14,306.60,395.20,251.09,5.99;14,306.60,403.79,251.09,5.94;14,306.60,412.35,251.09,5.94;14,306.60,420.92,251.09,5.94;14,306.60,429.49,251.09,5.94;14,306.60,438.06,50.60,5.94" xml:id="b55">
	<analytic>
		<title level="a" type="main">He is currently a Research Professor with the Institute of Image Communication and Information Processing</title>
		<author>
			<persName coords=""><forename type="first">Guangtao</forename><surname>Zhai Received The</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests include multimedia signal processing and perceptual signal processing</title>
				<meeting><address><addrLine>China; Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>degrees from Shandong University, Shandong ; D. degree from Shanghai Jiao Tong University ; Shanghai Jiao Tong University</orgName>
		</respStmt>
	</monogr>
	<note>2001 and 2004, respectively, and the Ph</note>
	<note type="raw_reference">Guangtao Zhai received the B.E. and M.E. degrees from Shandong University, Shan- dong, China, in 2001 and 2004, respectively, and the Ph.D. degree from Shanghai Jiao Tong University, Shanghai, China, in 2009. He is currently a Research Professor with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University. His research interests include multimedia signal processing and perceptual signal processing.</note>
</biblStruct>

<biblStruct coords="14,306.60,461.94,251.08,5.99;14,306.60,470.52,251.08,5.94;14,306.60,479.09,251.09,5.94;14,306.60,487.66,251.08,5.94;14,306.60,496.23,145.19,5.94" xml:id="b56">
	<analytic>
		<title level="a" type="main">Currently he is a research fellow with the school of Economics of Fudan University. He also serves as research fellow with Fudan-Stanford China Institute for Financial Technology and Security</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include: Blockchain, e-Government and Enterprise Information System</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Jingjing Chen received the Ph.D. in Computer Science from Hong Kong Baptist University</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Jingjing Chen received the Ph.D. in Computer Science from Hong Kong Baptist University in 2016. Currently he is a research fellow with the school of Economics of Fudan University. He also serves as research fellow with Fudan-Stanford China Institute for Financial Technology and Security. His research interests include: Blockchain, e-Government and Enterprise Information System.</note>
</biblStruct>

<biblStruct coords="14,306.60,520.11,251.08,5.99;14,306.60,528.69,251.08,5.94;14,306.60,537.26,251.09,5.94;14,306.60,545.83,251.08,5.94;14,306.60,554.40,54.53,5.94" xml:id="b57">
	<analytic>
		<title level="a" type="main">Currently he is an attending physician in the First Affiliated Hospital, Zhejiang University School of Medicine. His current research focuses on the diagnosis and treatment of emerging infectious diseases</title>
	</analytic>
	<monogr>
		<title level="m">2013, the Doctoral degree in Zhejiang University</title>
				<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Xiaoxin Wu received the B.E. degree from ChongQing Medical University</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Xiaoxin Wu received the B.E. degree from ChongQing Medical University, China, in 2013, the Doctoral degree in Zhejiang University, China, in 2019. Currently he is an attending physician in the First Affiliated Hospital, Zhejiang University School of Medicine. His current research focuses on the diagnosis and treatment of emerging infectious diseases.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
